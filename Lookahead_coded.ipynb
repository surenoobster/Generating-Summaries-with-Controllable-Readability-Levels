{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from sys import prefix\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "\n",
    "from transformers.generation.beam_constraints import Constraint, DisjunctiveConstraint, PhrasalConstraint\n",
    "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
    "from transformers.generation.logits_process import (\n",
    "    EncoderNoRepeatNGramLogitsProcessor,\n",
    "    ExponentialDecayLengthPenalty,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    LogitNormalization,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    TypicalLogitsWarper,\n",
    ")\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    MaxTimeCriteria,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "from transformers.utils import ModelOutput, logging\n",
    "'''from transformers.pytorch_utils import torch_int_div'''\n",
    "\n",
    "\n",
    "from transformers.generation.utils import (\n",
    "    GreedySearchEncoderDecoderOutput,\n",
    "    GreedySearchDecoderOnlyOutput,\n",
    "    BeamSearchEncoderDecoderOutput,\n",
    "    BeamSearchDecoderOnlyOutput,\n",
    "    SampleEncoderDecoderOutput,\n",
    "    SampleDecoderOnlyOutput,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import inspect\n",
    "from sys import prefix\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "\n",
    "from transformers.generation.beam_constraints import Constraint, DisjunctiveConstraint, PhrasalConstraint\n",
    "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
    "from transformers.generation.logits_process import (\n",
    "    EncoderNoRepeatNGramLogitsProcessor,\n",
    "    ExponentialDecayLengthPenalty,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    LogitNormalization,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    TypicalLogitsWarper,\n",
    ")\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    MaxTimeCriteria,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "from transformers.utils import ModelOutput, logging\n",
    "'''from transformers.pytorch_utils import torch_int_div'''\n",
    "\n",
    "\n",
    "from transformers.generation.utils import (\n",
    "    GenerateDecoderOnlyOutput,\n",
    "    GreedySearchEncoderDecoderOutput,\n",
    "    GreedySearchDecoderOnlyOutput,\n",
    "    BeamSearchEncoderDecoderOutput,\n",
    "    BeamSearchDecoderOnlyOutput,\n",
    "    SampleEncoderDecoderOutput,\n",
    "    SampleDecoderOnlyOutput,\n",
    "    GenerationMixin,\n",
    "    GenerateBeamDecoderOnlyOutput,\n",
    "    GenerateBeamEncoderDecoderOutput,\n",
    "    GenerateEncoderDecoderOutput,\n",
    "\n",
    ")\n",
    "from transformers.generation.utils import GenerateDecoderOnlyOutput\n",
    "\n",
    "\n",
    "# Typing shortcuts\n",
    "#BeamSearchEncoderDecoderOutput = GenerateBeamEncoderDecoderOutput\n",
    "#SampleEncoderDecoderOutput = GenerateEncoderDecoderOutput\n",
    "#SampleDecoderOnlyOutput = GenerateDecoderOnlyOutput\n",
    "BeamSampleEncoderDecoderOutput = GenerateBeamEncoderDecoderOutput\n",
    "GenerateNonBeamOutput = Union[GenerateDecoderOnlyOutput, GenerateEncoderDecoderOutput]\n",
    "GenerateBeamOutput = Union[GenerateBeamDecoderOnlyOutput, GenerateBeamEncoderDecoderOutput]\n",
    "GenerateOutput = Union[GenerateNonBeamOutput, GenerateBeamOutput]\n",
    "SampleOutput = Union[SampleEncoderDecoderOutput, SampleDecoderOnlyOutput]\n",
    "BeamSampleDecoderOnlyOutput = GenerateBeamDecoderOnlyOutput\n",
    "BeamSearchOutput = Union[BeamSearchEncoderDecoderOutput, BeamSearchDecoderOnlyOutput]\n",
    "BeamSampleOutput = Union[BeamSampleEncoderDecoderOutput, BeamSampleDecoderOnlyOutput]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/surenoobster/anaconda3/lib/python3.12/site-packages (4.34.0)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/surenoobster/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Using cached transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.14.1\n",
      "    Uninstalling tokenizers-0.14.1:\n",
      "      Successfully uninstalled tokenizers-0.14.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.34.0\n",
      "    Uninstalling transformers-4.34.0:\n",
      "      Successfully uninstalled transformers-4.34.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.27.1 tokenizers-0.21.0 transformers-4.47.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.47.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/surenoobster/anaconda3/lib/python3.12/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, trl\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (4155938290.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_4975/4155938290.py\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    ''''\u001b[0m\n\u001b[0m        \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "''''# TODO (joao): remove the equivalent classes and typing shortcuts below in v5\n",
    "# Equivalent classes (kept for retrocompatibility purposes)\n",
    "GreedySearchDecoderOnlyOutput = GenerateDecoderOnlyOutput\n",
    "ContrastiveSearchDecoderOnlyOutput = GenerateDecoderOnlyOutput\n",
    "SampleDecoderOnlyOutput = GenerateDecoderOnlyOutput\n",
    "\n",
    "ContrastiveSearchEncoderDecoderOutput = GenerateEncoderDecoderOutput\n",
    "GreedySearchEncoderDecoderOutput = GenerateEncoderDecoderOutput\n",
    "SampleEncoderDecoderOutput = GenerateEncoderDecoderOutput\n",
    "\n",
    "BeamSearchDecoderOnlyOutput = GenerateBeamDecoderOnlyOutput\n",
    "BeamSampleDecoderOnlyOutput = GenerateBeamDecoderOnlyOutput\n",
    "\n",
    "BeamSearchEncoderDecoderOutput = GenerateBeamEncoderDecoderOutput\n",
    "BeamSampleEncoderDecoderOutput = GenerateBeamEncoderDecoderOutput\n",
    "\n",
    "GreedySearchOutput = Union[GreedySearchEncoderDecoderOutput, GreedySearchDecoderOnlyOutput]\n",
    "SampleOutput = Union[SampleEncoderDecoderOutput, SampleDecoderOnlyOutput]\n",
    "BeamSearchOutput = Union[BeamSearchEncoderDecoderOutput, BeamSearchDecoderOnlyOutput]\n",
    "BeamSampleOutput = Union[BeamSampleEncoderDecoderOutput, BeamSampleDecoderOnlyOutput]\n",
    "ContrastiveSearchOutput = Union[ContrastiveSearchEncoderDecoderOutput, ContrastiveSearchDecoderOnlyOutput]\n",
    "\n",
    "# Typing shortcuts\n",
    "GenerateNonBeamOutput = Union[GenerateDecoderOnlyOutput, GenerateEncoderDecoderOutput]\n",
    "GenerateBeamOutput = Union[GenerateBeamDecoderOnlyOutput, GenerateBeamEncoderDecoderOutput]\n",
    "GenerateOutput = Union[GenerateNonBeamOutput, GenerateBeamOutput]\n",
    "\n",
    "''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.29.2\n",
      "Uninstalling transformers-4.29.2:\n",
      "  Successfully uninstalled transformers-4.29.2\n",
      "Collecting transformers==4.29.2\n",
      "  Using cached transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "Requirement already satisfied: filelock in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (3.12.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (0.13.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (6.7.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (0.16.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (2024.4.16)\n",
      "Requirement already satisfied: requests in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (24.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (2023.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from importlib-metadata->transformers==4.29.2) (3.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from requests->transformers==4.29.2) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from requests->transformers==4.29.2) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from requests->transformers==4.29.2) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from requests->transformers==4.29.2) (2.0.4)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers -y\n",
    "!pip install transformers==4.29.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.29.2\n",
      "  Using cached transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "Requirement already satisfied: rouge-score==0.1.2 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (0.1.2)\n",
      "Collecting accelerate==0.19.0\n",
      "  Using cached accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "Collecting datasets==2.12.0\n",
      "  Using cached datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "Requirement already satisfied: deepspeed==0.9.2 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (0.9.2)\n",
      "Requirement already satisfied: evaluate==0.4.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: py-readability-metrics==1.4.4 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (24.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (0.16.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (4.67.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (2024.4.16)\n",
      "Requirement already satisfied: filelock in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (3.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (1.21.5)\n",
      "Requirement already satisfied: importlib-metadata in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (6.7.0)\n",
      "Requirement already satisfied: requests in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers==4.29.2) (6.0.1)\n",
      "Requirement already satisfied: absl-py in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from rouge-score==0.1.2) (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from rouge-score==0.1.2) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from rouge-score==0.1.2) (1.16.0)\n",
      "Requirement already satisfied: psutil in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from accelerate==0.19.0) (6.1.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from accelerate==0.19.0) (1.13.1)\n",
      "Requirement already satisfied: pandas in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets==2.12.0) (1.3.5)\n",
      "Requirement already satisfied: aiohttp in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets==2.12.0) (3.8.6)\n",
      "Requirement already satisfied: responses<0.19 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets==2.12.0) (0.18.0)\n",
      "Requirement already satisfied: multiprocess in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets==2.12.0) (0.70.14)\n",
      "Requirement already satisfied: xxhash in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets==2.12.0) (3.5.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets==2.12.0) (0.3.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets==2.12.0) (2023.1.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets==2.12.0) (12.0.1)\n",
      "Requirement already satisfied: hjson in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from deepspeed==0.9.2) (3.1.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from deepspeed==0.9.2) (9.0.0)\n",
      "Requirement already satisfied: ninja in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from deepspeed==0.9.2) (1.11.1.2)\n",
      "Requirement already satisfied: pydantic<2.0.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from deepspeed==0.9.2) (1.10.19)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (2.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (4.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (6.0.5)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (24.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (1.9.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from requests->transformers==4.29.2) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from requests->transformers==4.29.2) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from requests->transformers==4.29.2) (1.26.14)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate==0.19.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate==0.19.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate==0.19.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate==0.19.0) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate==0.19.0) (65.6.3)\n",
      "Requirement already satisfied: wheel in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate==0.19.0) (0.38.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from importlib-metadata->transformers==4.29.2) (3.15.0)\n",
      "Requirement already satisfied: click in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from nltk->rouge-score==0.1.2) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from nltk->rouge-score==0.1.2) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from pandas->datasets==2.12.0) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from pandas->datasets==2.12.0) (2.9.0.post0)\n",
      "Installing collected packages: transformers, accelerate, datasets\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.30.2\n",
      "    Uninstalling transformers-4.30.2:\n",
      "      Successfully uninstalled transformers-4.30.2\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.20.3\n",
      "    Uninstalling accelerate-0.20.3:\n",
      "      Successfully uninstalled accelerate-0.20.3\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.13.2\n",
      "    Uninstalling datasets-2.13.2:\n",
      "      Successfully uninstalled datasets-2.13.2\n",
      "Successfully installed accelerate-0.19.0 datasets-2.12.0 transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.29.2 rouge-score==0.1.2 accelerate==0.19.0 datasets==2.12.0 deepspeed==0.9.2 evaluate==0.4.0 py-readability-metrics==1.4.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Lookahead:\n",
    "    \"\"\"\n",
    "    Object that performs the lookahead. This is very similar to GenerationMixin, since it needs to decode the sequence as well,\n",
    "    but this contains the additional function to compute heuristics score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        scorer,\n",
    "        lookahead_length=1,\n",
    "        lookahead_lambda=1.0,\n",
    "        lookahead_top_k=5,\n",
    "        decoding_type=\"greedy\",\n",
    "        max_length: Optional[int] = None,\n",
    "        min_length: Optional[int] = None,\n",
    "        do_sample: Optional[bool] = None,\n",
    "        early_stopping: Optional[bool] = None,\n",
    "        num_beams: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        typical_p: Optional[float] = None,\n",
    "        repetition_penalty: Optional[float] = None,\n",
    "        bad_words_ids: Optional[Iterable[int]] = None,\n",
    "        force_words_ids: Optional[Union[Iterable[int], Iterable[Iterable[int]]]] = None,\n",
    "        bos_token_id: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        length_penalty: Optional[float] = None,\n",
    "        no_repeat_ngram_size: Optional[int] = None,\n",
    "        encoder_no_repeat_ngram_size: Optional[int] = None,\n",
    "        num_return_sequences: Optional[int] = None,\n",
    "        max_time: Optional[float] = None,\n",
    "        max_new_tokens: Optional[int] = None,\n",
    "        decoder_start_token_id: Optional[int] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        num_beam_groups: Optional[int] = None,\n",
    "        diversity_penalty: Optional[float] = None,\n",
    "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "        logits_processor: Optional[LogitsProcessorList] = LogitsProcessorList(),\n",
    "        renormalize_logits: Optional[bool] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = StoppingCriteriaList(),\n",
    "        constraints: Optional[List[Constraint]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        forced_bos_token_id: Optional[int] = None,\n",
    "        forced_eos_token_id: Optional[int] = None,\n",
    "        remove_invalid_values: Optional[bool] = None,\n",
    "        synced_gpus: Optional[bool] = False,\n",
    "        exponential_decay_length_penalty: Optional[Tuple[Union[int, float]]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        model: The Huggingface Model\n",
    "        tokenizer: The tokenizer for decoding the summaries\n",
    "        scorer: Scorer object that calculates the score given document and summary\n",
    "        lookahead_length: The number of tokens to look ahead\n",
    "        lookahead_lambda: The weight for the score\n",
    "        lookahead_top_k: The number of top tokens to consider for expansion\n",
    "        decoding_type: The decoding type for lookahead. [greedy, beam, sample]\n",
    "\n",
    "        Other parameters are the same arguments expected for GenerationMixin to control the generation\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.scorer = scorer\n",
    "\n",
    "        if lookahead_length == -1:\n",
    "            assert max_length is not None\n",
    "            self.lookahead_length = max_length\n",
    "            self.lookahead_until_sent = True\n",
    "        else:\n",
    "            self.lookahead_length = lookahead_length\n",
    "            self.lookahead_until_sent = False\n",
    "        \n",
    "        self.lookahead_lambda = lookahead_lambda\n",
    "        self.lookahead_top_k = lookahead_top_k\n",
    "        self.decoding_type = decoding_type\n",
    "\n",
    "        if self.decoding_type == \"greedy\":\n",
    "            self.decoding_func = self.greedy_search\n",
    "        elif self.decoding_type == \"beam\":\n",
    "            self.decoding_func = self.beam_search\n",
    "        elif self.decoding_type == \"sample\":\n",
    "            self.decoding_func = self.sample\n",
    "\n",
    "        # generation parameters from generate()\n",
    "        self.bos_token_id = self.model.config.bos_token_id\n",
    "        self.num_beams = num_beams if num_beams is not None else self.model.config.num_beams\n",
    "        self.length_penalty = length_penalty if length_penalty is not None else self.model.config.length_penalty\n",
    "        self.early_stopping = early_stopping if early_stopping is not None else self.model.config.early_stopping\n",
    "        self.num_beam_groups = num_beam_groups if num_beam_groups is not None else self.model.config.num_beam_groups\n",
    "        self.num_return_sequences = (\n",
    "            num_return_sequences if num_return_sequences is not None else self.model.config.num_return_sequences\n",
    "        )\n",
    "\n",
    "        self.pad_token_id = self.model.config.pad_token_id\n",
    "        self.eos_token_id = self.model.config.eos_token_id\n",
    "\n",
    "        if self.eos_token_id is None and hasattr(self.model.config, \"decoder\"):\n",
    "            self.eos_token_id = self.model.config.decoder.eos_token_id\n",
    "\n",
    "        if self.pad_token_id is None and self.eos_token_id is not None:\n",
    "            # special case if pad_token_id is not defined\n",
    "            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{self.eos_token_id} for open-end generation.\")\n",
    "            self.pad_token_id = self.eos_token_id\n",
    "        self.max_length =  max_length\n",
    "        self.min_length = min_length\n",
    "        self.temperature = temperature\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.typical_p = typical_p\n",
    "        self.reptition_penality = repetition_penalty\n",
    "        self.bad_words_ids = bad_words_ids\n",
    "        self.force_words_ids = force_words_ids\n",
    "        self.no_repeat_ngram_size = no_repeat_ngram_size\n",
    "        self.encoder_no_repeat_ngram_size = encoder_no_repeat_ngram_size\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.decoder_start_token_id = decoder_start_token_id\n",
    "        self.use_cache = use_cache\n",
    "        self.diversity_penalty = diversity_penalty\n",
    "        self.prefix_allowed_tokens_fn = prefix_allowed_tokens_fn\n",
    "        self.renormalize_logits = renormalize_logits\n",
    "        self.contraints = constraints\n",
    "        self.forced_bos_token_id = forced_bos_token_id\n",
    "        self.forced_eos_token_id = forced_eos_token_id\n",
    "        self.remove_invalid_values = remove_invalid_values\n",
    "        self.exponential_decay_length_penalty = exponential_decay_length_penalty\n",
    "        self.synced_gpus = synced_gpus\n",
    "\n",
    "        # self.return_dict_in_generate = return_dict_in_generate\n",
    "        self.return_dict_in_generate = True\n",
    "        self.output_attentions = output_attentions\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "        self.output_scores = output_scores\n",
    "\n",
    "        # If not provided, logits processor will be prepared later since it requires input_tensor\n",
    "        self.logits_processor = logits_processor\n",
    "\n",
    "        # prepare stopping criteria\n",
    "        self.stopping_criteria = self.model._get_stopping_criteria(\n",
    "            max_length=max_length, max_time=max_time, stopping_criteria=stopping_criteria\n",
    "        )\n",
    "\n",
    "        self.logits_warper = self.model._get_logits_warper(\n",
    "            top_k=self.top_k,\n",
    "            top_p=self.top_p,\n",
    "            typical_p=self.typical_p,\n",
    "            temperature=self.temperature,\n",
    "            num_beams=self.num_beams,\n",
    "            renormalize_logits=self.renormalize_logits,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(\n",
    "    self,\n",
    "    input_ids,\n",
    "    next_token_scores,\n",
    "    num_beams=1,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to call for the lookahead. This function generates the sequences and returns the calculated heuristics.\n",
    "    \"\"\"\n",
    "    # Prepare for generation\n",
    "    if self.logits_processor is None:\n",
    "        input_ids_seq_length = input_ids.size(1)\n",
    "        inputs_tensor = model_kwargs[\"encoder_outputs\"][self.model.main_input_name]\n",
    "\n",
    "        self.logits_processor = self.model._get_logits_processor(\n",
    "            repetition_penalty=self.repetition_penalty,\n",
    "            no_repeat_ngram_size=self.no_repeat_ngram_size,\n",
    "            encoder_no_repeat_ngram_size=self.encoder_no_repeat_ngram_size,\n",
    "            input_ids_seq_length=input_ids_seq_length,\n",
    "            encoder_input_ids=inputs_tensor,\n",
    "            bad_words_ids=self.bad_words_ids,\n",
    "            min_length=self.min_length,\n",
    "            max_length=self.max_length,\n",
    "            eos_token_id=self.eos_token_id,\n",
    "            forced_bos_token_id=self.forced_bos_token_id,\n",
    "            forced_eos_token_id=self.forced_eos_token_id,\n",
    "            prefix_allowed_tokens_fn=self.prefix_allowed_tokens_fn,\n",
    "            num_beams=num_beams,\n",
    "            num_beam_groups=self.num_beam_groups,\n",
    "            diversity_penalty=self.diversity_penalty,\n",
    "            remove_invalid_values=self.remove_invalid_values,\n",
    "            exponential_decay_length_penalty=self.exponential_decay_length_penalty,\n",
    "            logits_processor=self.logits_processor,\n",
    "            renormalize_logits=self.renormalize_logits,\n",
    "        )\n",
    "\n",
    "    do_sample = \"sample\" in self.decoding_type\n",
    "    use_beam = \"beam\" in self.decoding_type\n",
    "    beam_scorer = None\n",
    "\n",
    "    if use_beam:\n",
    "        batch_size = input_ids.shape[0] * self.lookahead_top_k\n",
    "        beam_scorer = BeamSearchScorer(\n",
    "            batch_size=batch_size,\n",
    "            num_beams=num_beams,\n",
    "            max_length=self.stopping_criteria.max_length,\n",
    "            device=input_ids.device,\n",
    "            length_penalty=self.length_penalty,\n",
    "            do_early_stopping=self.early_stopping,\n",
    "            num_beam_hyps_to_keep=self.num_return_sequences,\n",
    "            num_beam_groups=self.num_beam_groups,\n",
    "        )\n",
    "\n",
    "    indices = torch.arange(input_ids.size(0), dtype=input_ids.dtype, device=input_ids.device)\n",
    "\n",
    "    # Expand for top-k tokens to use with scorer\n",
    "    _, top_k_indices = torch.topk(next_token_scores, k=self.lookahead_top_k, dim=-1)\n",
    "    top_k_indices = top_k_indices.reshape(-1)\n",
    "\n",
    "    indices = indices.repeat_interleave(self.lookahead_top_k)\n",
    "    input_ids = torch.cat([input_ids[indices], top_k_indices.unsqueeze(1)], dim=1)\n",
    "\n",
    "    # Adjust model_kwargs\n",
    "    model_kwargs = self.expand_model_kwargs(model_kwargs, indices)\n",
    "\n",
    "    # Expand if necessary for beam search\n",
    "    if use_beam:\n",
    "        input_ids, model_kwargs = self.model._expand_inputs_for_generation(\n",
    "            input_ids,\n",
    "            expand_size=num_beams,\n",
    "            is_encoder_decoder=self.model.config.is_encoder_decoder,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        indices = indices.repeat_interleave(num_beams)\n",
    "        if \"past\" in model_kwargs:\n",
    "            model_kwargs[\"past\"] = tuple(\n",
    "                tuple(p.repeat_interleave(num_beams, dim=0) for p in past)\n",
    "                for past in model_kwargs[\"past\"]\n",
    "            )\n",
    "\n",
    "    # Generate sequences\n",
    "    if self.lookahead_length == 0:\n",
    "        seq = input_ids\n",
    "    else:\n",
    "        dec_out = self.decoding_func(input_ids, beam_scorer, **model_kwargs)\n",
    "        seq = dec_out[\"sequences\"]\n",
    "\n",
    "    # Decode sequences\n",
    "    dec_seq = self.tokenizer.batch_decode(seq, skip_special_tokens=True)\n",
    "\n",
    "    # Calculate scores\n",
    "    _lookahead_scores = self.scorer.score(dec_seq, indices // num_beams)\n",
    "    _lookahead_scores = torch.clamp(_lookahead_scores, min=1e-9).log()\n",
    "\n",
    "    _lookahead_scores = _lookahead_scores.view(-1, self.lookahead_top_k, num_beams)\n",
    "    _lookahead_scores, _ = _lookahead_scores.max(-1)\n",
    "\n",
    "    lookahead_scores = torch.full_like(next_token_scores, fill_value=1e-9, dtype=_lookahead_scores.dtype, device=next_token_scores.device).log()\n",
    "\n",
    "    next_token_scores = F.log_softmax(next_token_scores, dim=-1)\n",
    "\n",
    "    if use_beam:\n",
    "        indices = indices.view(-1, num_beams)[:, 0]\n",
    "\n",
    "    lookahead_scores[indices, top_k_indices] = _lookahead_scores.view(-1)\n",
    "\n",
    "    return self.lookahead_lambda * lookahead_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor,\n",
    "    beam_scorer=None,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    # Initialize attention/hidden states/scores tuples\n",
    "    scores = () if (self.return_dict_in_generate and self.output_scores) else None\n",
    "    decoder_attentions = () if (self.return_dict_in_generate and self.output_attentions) else None\n",
    "    cross_attentions = () if (self.return_dict_in_generate and self.output_attentions) else None\n",
    "    decoder_hidden_states = () if (self.return_dict_in_generate and self.output_hidden_states) else None\n",
    "\n",
    "    # If model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "    if self.return_dict_in_generate and self.model.config.is_encoder_decoder:\n",
    "        encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if self.output_attentions else None\n",
    "        encoder_hidden_states = (\n",
    "            model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if self.output_hidden_states else None\n",
    "        )\n",
    "\n",
    "    # Keep track of which sequences are already finished\n",
    "    unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "    cur_len = input_ids.shape[-1]\n",
    "\n",
    "    lookahead_length = self.lookahead_length + cur_len\n",
    "\n",
    "    this_peer_finished = False  # Used by synced_gpus only\n",
    "    while True:\n",
    "        if self.synced_gpus:\n",
    "            # Under synced_gpus, the `forward` call must continue until all GPUs complete their sequence.\n",
    "            # The following logic allows an early break if all peers finished generating their sequence\n",
    "            this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
    "            # Send 0.0 if we finished, 1.0 otherwise\n",
    "            dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
    "            # Did all peers finish? The reduced sum will be 0.0 then\n",
    "            if this_peer_finished_flag.item() == 0.0:\n",
    "                break\n",
    "\n",
    "        # Prepare model inputs\n",
    "        model_inputs = self.model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "        # Forward pass to get next token\n",
    "        outputs = self.model(\n",
    "            **model_inputs,\n",
    "            return_dict=True,\n",
    "            output_attentions=self.output_attentions,\n",
    "            output_hidden_states=self.output_hidden_states,\n",
    "        )\n",
    "\n",
    "        if self.synced_gpus and this_peer_finished:\n",
    "            cur_len = cur_len + 1\n",
    "            continue  # Don't waste resources running the code we don't need\n",
    "\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Store scores, attentions, and hidden_states when required\n",
    "        if self.return_dict_in_generate:\n",
    "            if self.output_scores:\n",
    "                scores += (next_token_logits,)\n",
    "            if self.output_attentions:\n",
    "                decoder_attentions += (\n",
    "                    (outputs.decoder_attentions,)\n",
    "                    if self.model.config.is_encoder_decoder\n",
    "                    else (outputs.attentions,)\n",
    "                )\n",
    "                if self.model.config.is_encoder_decoder:\n",
    "                    cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "            if self.output_hidden_states:\n",
    "                decoder_hidden_states += (\n",
    "                    (outputs.decoder_hidden_states,)\n",
    "                    if self.model.config.is_encoder_decoder\n",
    "                    else (outputs.hidden_states,)\n",
    "                )\n",
    "\n",
    "        # Pre-process distribution\n",
    "        next_tokens_scores = self.logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "        # Argmax\n",
    "        next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n",
    "\n",
    "        # Finished sentences should have their next token be a padding token\n",
    "        if self.eos_token_id is not None:\n",
    "            if self.pad_token_id is None:\n",
    "                raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
    "            next_tokens = next_tokens * unfinished_sequences + self.pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "        # Update generated ids, model inputs, and length for next step\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "        model_kwargs = self.model._update_model_kwargs_for_generation(\n",
    "            outputs, model_kwargs, is_encoder_decoder=self.model.config.is_encoder_decoder\n",
    "        )\n",
    "        cur_len = cur_len + 1\n",
    "\n",
    "        # Lookahead break\n",
    "        if cur_len >= lookahead_length:\n",
    "            break\n",
    "\n",
    "        # If eos_token was found in one sentence, set sentence to finished\n",
    "        if self.eos_token_id is not None:\n",
    "            unfinished_sequences = unfinished_sequences.mul((next_tokens != self.eos_token_id).long())\n",
    "\n",
    "        # Stop when each sentence is finished, or if we exceed the maximum length\n",
    "        if unfinished_sequences.max() == 0 or self.stopping_criteria(input_ids, scores):\n",
    "            if not self.synced_gpus:\n",
    "                break\n",
    "            else:\n",
    "                this_peer_finished = True\n",
    "\n",
    "    if self.return_dict_in_generate:\n",
    "        if self.model.config.is_encoder_decoder:\n",
    "            return GreedySearchEncoderDecoderOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                encoder_attentions=encoder_attentions,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                decoder_attentions=decoder_attentions,\n",
    "                cross_attentions=cross_attentions,\n",
    "                decoder_hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return GreedySearchDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                attentions=decoder_attentions,\n",
    "                hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "    else:\n",
    "        return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor,\n",
    "    beam_scorer=None,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    batch_size = len(beam_scorer._beam_hyps)\n",
    "    num_beams = beam_scorer.num_beams\n",
    "\n",
    "    batch_beam_size, cur_len = input_ids.shape\n",
    "    lookahead_length = self.lookahead_length + cur_len\n",
    "\n",
    "    if num_beams * batch_size != batch_beam_size:\n",
    "        raise ValueError(\n",
    "            f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
    "        )\n",
    "\n",
    "    # Initialize attention/hidden states/scores tuples\n",
    "    scores = () if (self.return_dict_in_generate and self.output_scores) else None\n",
    "    beam_indices = (\n",
    "        tuple(() for _ in range(batch_beam_size))\n",
    "        if (self.return_dict_in_generate and self.output_scores)\n",
    "        else None\n",
    "    )\n",
    "    decoder_attentions = () if (self.return_dict_in_generate and self.output_attentions) else None\n",
    "    cross_attentions = () if (self.return_dict_in_generate and self.output_attentions) else None\n",
    "    decoder_hidden_states = () if (self.return_dict_in_generate and self.output_hidden_states) else None\n",
    "\n",
    "    # If model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "    if self.return_dict_in_generate and self.model.config.is_encoder_decoder:\n",
    "        encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if self.output_attentions else None\n",
    "        encoder_hidden_states = (\n",
    "            model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if self.output_hidden_states else None\n",
    "        )\n",
    "\n",
    "    beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
    "    beam_scores[:, 1:] = -1e9\n",
    "    beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "    this_peer_finished = False  # Used by synced_gpus only\n",
    "    while True:\n",
    "\n",
    "        if self.synced_gpus:\n",
    "            this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
    "            dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
    "            if this_peer_finished_flag.item() == 0.0:\n",
    "                break\n",
    "\n",
    "        model_inputs = self.model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "        outputs = self.model(\n",
    "            **model_inputs,\n",
    "            return_dict=True,\n",
    "            output_attentions=self.output_attentions,\n",
    "            output_hidden_states=self.output_hidden_states,\n",
    "        )\n",
    "\n",
    "        if self.synced_gpus and this_peer_finished:\n",
    "            cur_len += 1\n",
    "            continue\n",
    "\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token_logits = self.model.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)\n",
    "        next_token_scores = nn.functional.log_softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        next_token_scores_processed = self.logits_processor(input_ids, next_token_scores)\n",
    "        next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(next_token_scores)\n",
    "\n",
    "        if self.return_dict_in_generate:\n",
    "            if self.output_scores:\n",
    "                scores += (next_token_scores_processed,)\n",
    "            if self.output_attentions:\n",
    "                decoder_attentions += (\n",
    "                    (outputs.decoder_attentions,) if self.model.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                )\n",
    "                if self.model.config.is_encoder_decoder:\n",
    "                    cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "            if self.output_hidden_states:\n",
    "                decoder_hidden_states += (\n",
    "                    (outputs.decoder_hidden_states,)\n",
    "                    if self.model.config.is_encoder_decoder\n",
    "                    else (outputs.hidden_states,)\n",
    "                )\n",
    "\n",
    "        vocab_size = next_token_scores.shape[-1]\n",
    "        next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
    "\n",
    "        next_token_scores, next_tokens = torch.topk(\n",
    "            next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True\n",
    "        )\n",
    "\n",
    "        next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
    "        next_tokens = next_tokens % vocab_size\n",
    "\n",
    "        beam_outputs = beam_scorer.process(\n",
    "            input_ids,\n",
    "            next_token_scores,\n",
    "            next_tokens,\n",
    "            next_indices,\n",
    "            pad_token_id=self.pad_token_id,\n",
    "            eos_token_id=self.eos_token_id,\n",
    "        )\n",
    "\n",
    "        beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "        beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "        beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "        input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        model_kwargs = self.model._update_model_kwargs_for_generation(\n",
    "            outputs, model_kwargs, is_encoder_decoder=self.model.config.is_encoder_decoder\n",
    "        )\n",
    "        if model_kwargs.get(\"past\") is not None:\n",
    "            model_kwargs[\"past\"] = self.model._reorder_cache(model_kwargs[\"past\"], beam_idx)\n",
    "\n",
    "        if self.return_dict_in_generate and self.output_scores:\n",
    "            beam_indices = tuple((beam_indices[beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices))))\n",
    "\n",
    "        cur_len += 1\n",
    "\n",
    "        if cur_len >= lookahead_length:\n",
    "            break\n",
    "\n",
    "        if beam_scorer.is_done or self.stopping_criteria(input_ids, scores):\n",
    "            if not self.synced_gpus:\n",
    "                break\n",
    "            else:\n",
    "                this_peer_finished = True\n",
    "\n",
    "    sequence_outputs = beam_scorer.finalize(\n",
    "        input_ids,\n",
    "        beam_scores,\n",
    "        next_tokens,\n",
    "        next_indices,\n",
    "        pad_token_id=self.pad_token_id,\n",
    "        eos_token_id=self.eos_token_id,\n",
    "        max_length=self.stopping_criteria.max_length,\n",
    "    )\n",
    "\n",
    "    if self.return_dict_in_generate:\n",
    "        if not self.output_scores:\n",
    "            sequence_outputs[\"sequence_scores\"] = None\n",
    "        else:\n",
    "            num_return_sequences = beam_scorer.num_beam_hyps_to_keep\n",
    "            beam_indices = tuple(\n",
    "                (beam_indices[i * num_beams: i * num_beams + num_return_sequences] for i in range(batch_size))\n",
    "            )\n",
    "            beam_indices = sum(beam_indices, ())\n",
    "\n",
    "        if self.model.config.is_encoder_decoder:\n",
    "            return BeamSearchEncoderDecoderOutput(\n",
    "                sequences=sequence_outputs[\"sequences\"],\n",
    "                sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                scores=scores,\n",
    "                beam_indices=beam_indices,\n",
    "                encoder_attentions=encoder_attentions,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                decoder_attentions=decoder_attentions,\n",
    "                cross_attentions=cross_attentions,\n",
    "                decoder_hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return BeamSearchDecoderOnlyOutput(\n",
    "                sequences=sequence_outputs[\"sequences\"],\n",
    "                sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                scores=scores,\n",
    "                beam_indices=beam_indices,\n",
    "                attentions=decoder_attentions,\n",
    "                hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "    else:\n",
    "        return sequence_outputs[\"sequences\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor,\n",
    "    beam_scorer=None,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    scores = () if (self.return_dict_in_generate and self.output_scores) else None\n",
    "    decoder_attentions = () if (self.return_dict_in_generate and self.output_attentions) else None\n",
    "    cross_attentions = () if (self.return_dict_in_generate and self.output_attentions) else None\n",
    "    decoder_hidden_states = () if (self.return_dict_in_generate and self.output_hidden_states) else None\n",
    "\n",
    "    # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "    if self.return_dict_in_generate and self.model.config.is_encoder_decoder:\n",
    "        encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if self.output_attentions else None\n",
    "        encoder_hidden_states = (\n",
    "            model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if self.output_hidden_states else None\n",
    "        )\n",
    "\n",
    "    # keep track of which sequences are already finished\n",
    "    unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "    cur_len = input_ids.shape[-1]\n",
    "\n",
    "    lookahead_length = self.lookahead_length + cur_len\n",
    "\n",
    "    this_peer_finished = False  # used by synced_gpus only\n",
    "    # auto-regressive generation\n",
    "    while True:\n",
    "\n",
    "        if self.synced_gpus:\n",
    "            # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n",
    "            # The following logic allows an early break if all peers finished generating their sequence\n",
    "            this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
    "            # send 0.0 if we finished, 1.0 otherwise\n",
    "            dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
    "            # did all peers finish? the reduced sum will be 0.0 then\n",
    "            if this_peer_finished_flag.item() == 0.0:\n",
    "                break\n",
    "\n",
    "        # prepare model inputs\n",
    "        model_inputs = self.model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "        # forward pass to get next token\n",
    "        outputs = self.model(\n",
    "            **model_inputs,\n",
    "            return_dict=True,\n",
    "            output_attentions=self.output_attentions,\n",
    "            output_hidden_states=self.output_hidden_states,\n",
    "        )\n",
    "\n",
    "        if self.synced_gpus and this_peer_finished:\n",
    "            cur_len = cur_len + 1\n",
    "            continue  # don't waste resources running the code we don't need\n",
    "\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # pre-process distribution\n",
    "        next_token_scores = self.logits_processor(input_ids, next_token_logits)\n",
    "        next_token_scores = self.logits_warper(input_ids, next_token_scores)\n",
    "\n",
    "        # Store scores, attentions and hidden_states when required\n",
    "        if self.return_dict_in_generate:\n",
    "            if self.output_scores:\n",
    "                scores += (next_token_scores,)\n",
    "            if self.output_attentions:\n",
    "                decoder_attentions += (\n",
    "                    (outputs.decoder_attentions,) if self.model.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                )\n",
    "                if self.model.config.is_encoder_decoder:\n",
    "                    cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "            if self.output_hidden_states:\n",
    "                decoder_hidden_states += (\n",
    "                    (outputs.decoder_hidden_states,)\n",
    "                    if self.model.config.is_encoder_decoder\n",
    "                    else (outputs.hidden_states,)\n",
    "                )\n",
    "\n",
    "        # sample\n",
    "        probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "\n",
    "        # finished sentences should have their next token be a padding token\n",
    "        if self.eos_token_id is not None:\n",
    "            if self.pad_token_id is None:\n",
    "                raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
    "            next_tokens = next_tokens * unfinished_sequences + self.pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "        # update generated ids, model inputs, and length for next step\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "        model_kwargs = self.model._update_model_kwargs_for_generation(\n",
    "            outputs, model_kwargs, is_encoder_decoder=self.model.config.is_encoder_decoder\n",
    "        )\n",
    "        cur_len = cur_len + 1\n",
    "\n",
    "        if cur_len >= lookahead_length:\n",
    "            break\n",
    "\n",
    "        # if eos_token was found in one sentence, set sentence to finished\n",
    "        if self.eos_token_id is not None:\n",
    "            unfinished_sequences = unfinished_sequences.mul((next_tokens != self.eos_token_id).long())\n",
    "\n",
    "        # stop when each sentence is finished, or if we exceed the maximum length\n",
    "        if unfinished_sequences.max() == 0 or self.stopping_criteria(input_ids, scores):\n",
    "            if not self.synced_gpus:\n",
    "                break\n",
    "            else:\n",
    "                this_peer_finished = True\n",
    "\n",
    "    if self.return_dict_in_generate:\n",
    "        if self.model.config.is_encoder_decoder:\n",
    "            return SampleEncoderDecoderOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                encoder_attentions=encoder_attentions,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                decoder_attentions=decoder_attentions,\n",
    "                cross_attentions=cross_attentions,\n",
    "                decoder_hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            return SampleDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                attentions=decoder_attentions,\n",
    "                hidden_states=decoder_hidden_states,\n",
    "            )\n",
    "    else:\n",
    "        return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_model_kwargs(self, model_kwargs, indices):\n",
    "        model_kwargs = copy.deepcopy(model_kwargs)\n",
    "        if \"attention_mask\" in model_kwargs:\n",
    "            model_kwargs[\"attention_mask\"] = model_kwargs[\"attention_mask\"][indices]\n",
    "        if \"encoder_outputs\" in model_kwargs:\n",
    "            for k,v in model_kwargs[\"encoder_outputs\"].items():\n",
    "                if v is not None:\n",
    "                    model_kwargs[\"encoder_outputs\"][k] = v[indices]\n",
    "        if \"past\" in model_kwargs:\n",
    "            model_kwargs[\"past\"] = tuple([tuple([p[indices] for p in past]) for past in model_kwargs[\"past\"]])\n",
    "        return model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
