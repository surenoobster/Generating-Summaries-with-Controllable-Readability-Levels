{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'input_metrics', 'summary', 'id', 'prompt', 'input_noprompt'],\n",
      "    num_rows: 286\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Load your dataset (ensure you have the correct path)\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f.readlines()]\n",
    "    return data\n",
    "\n",
    "# Load the train data\n",
    "train_data = load_json_data('/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/train_metrics_np_summary_prompt_category.json')\n",
    "\n",
    "# Convert list of dictionaries to a dictionary of lists for the dataset format\n",
    "def convert_to_dict_of_lists(data):\n",
    "    dict_data = {}\n",
    "    for entry in data:\n",
    "        for key, value in entry.items():\n",
    "            if key not in dict_data:\n",
    "                dict_data[key] = []\n",
    "            dict_data[key].append(value)\n",
    "    return dict_data\n",
    "\n",
    "train_dict = convert_to_dict_of_lists(train_data)\n",
    "\n",
    "# Create the dataset from the converted dictionary\n",
    "train_dataset = Dataset.from_dict(train_dict)\n",
    "\n",
    "# Optionally print the dataset to verify\n",
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# Copyright 2021 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning the library models for sequence to sequence.\n",
    "\"\"\"\n",
    "# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import nltk  # Here to have a nice missing dependency error message early on\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "import evaluate\n",
    "import transformers\n",
    "from filelock import FileLock\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    MBart50Tokenizer,\n",
    "    MBart50TokenizerFast,\n",
    "    MBartTokenizer,\n",
    "    MBartTokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, is_offline_mode, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "#check_min_version(\"4.25.0.dev0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/summarization/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    if is_offline_mode():\n",
    "        raise LookupError(\n",
    "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
    "        )\n",
    "    with FileLock(\".lock\") as lock:\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# A list of all multilingual tokenizer which require lang attribute.\n",
    "MULTILINGUAL_TOKENIZERS = [MBartTokenizer, MBartTokenizerFast, MBart50Tokenizer, MBart50TokenizerFast]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    resize_position_embeddings: Optional[bool] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to automatically resize the position embeddings if `max_source_length` exceeds \"\n",
    "                \"the model's position embeddings.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    lang: Optional[str] = field(default=None, metadata={\"help\": \"Language id for summarization.\"})\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    text_column: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n",
    "    )\n",
    "    summary_column: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a jsonlines or csv file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"An optional input evaluation data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional input test data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    max_source_length: Optional[int] = field(\n",
    "        default=1024,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_target_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    val_max_target_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n",
    "                \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n",
    "                \"during ``evaluate`` and ``predict``.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to pad all samples to model maximum sentence length. \"\n",
    "                \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n",
    "                \"efficient on GPU but very bad for TPU.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    num_beams: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n",
    "                \"which is used during ``evaluate`` and ``predict``.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    ignore_pad_token_for_loss: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n",
    "        },\n",
    "    )\n",
    "    source_prefix: Optional[str] = field(\n",
    "        default=\"\", metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n",
    "    )\n",
    "\n",
    "    forced_bos_token: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The token to force as the first generated token after the decoder_start_token_id.\"\n",
    "                \"Useful for multilingual models like mBART where the first generated token\"\n",
    "                \"needs to be the target language token (Usually it is the target language token)\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "        if self.val_max_target_length is None:\n",
    "            self.val_max_target_length = self.max_target_length\n",
    "\n",
    "\n",
    "summarization_name_mapping = {\n",
    "    \"amazon_reviews_multi\": (\"review_body\", \"review_title\"),\n",
    "    \"big_patent\": (\"description\", \"abstract\"),\n",
    "    \"cnn_dailymail\": (\"article\", \"highlights\"),\n",
    "    \"orange_sum\": (\"text\", \"summary\"),\n",
    "    \"pn_summary\": (\"article\", \"summary\"),\n",
    "    \"psc\": (\"extract_text\", \"summary_text\"),\n",
    "    \"samsum\": (\"dialogue\", \"summary\"),\n",
    "    \"thaisum\": (\"body\", \"summary\"),\n",
    "    \"xglue\": (\"news_body\", \"news_title\"),\n",
    "    \"xsum\": (\"document\", \"summary\"),\n",
    "    \"wiki_summary\": (\"article\", \"highlights\"),\n",
    "    \"multi_news\": (\"document\", \"summary\"),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_id=\"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    " \n",
    "model_id=\"google/flan-t5-base\"\n",
    " \n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61551aad16db45a7baa4c0471e918c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24415/2706047411.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google/flan-t5-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RMWeerasinghe/flan-t5-base-prompt_tuning-cnn-dailymail\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             return model_class.from_pretrained(\n\u001b[0;32m--> 485\u001b[0;31m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             )\n\u001b[1;32m    487\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2492\u001b[0m                         \u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m                     }\n\u001b[0;32m-> 2494\u001b[0;31m                     \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcached_file_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m                     \u001b[0;31m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         )\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                 \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                 \u001b[0mexpected_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m             )\n\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEffectiveLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     )\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m                 \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "model = PeftModel.from_pretrained(base_model, \"RMWeerasinghe/flan-t5-base-prompt_tuning-cnn-dailymail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (0.3.10)\n",
      "Requirement already satisfied: transformers in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (4.30.2)\n",
      "Requirement already satisfied: datasets in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (2.13.2)\n",
      "Requirement already satisfied: rouge-score in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: tensorboard in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (2.11.2)\n",
      "Requirement already satisfied: py7zr in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (0.20.6)\n",
      "Collecting py7zr\n",
      "  Using cached py7zr-0.21.1-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from pytesseract) (24.0)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from pytesseract) (9.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: requests in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers) (6.7.0)\n",
      "Requirement already satisfied: filelock in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: xxhash in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: multiprocess in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pandas in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: absl-py in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: click in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from tensorboard) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from tensorboard) (2.37.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from tensorboard) (2.2.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from tensorboard) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from tensorboard) (65.6.3)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from tensorboard) (1.62.3)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from py7zr) (0.2.3)\n",
      "  Using cached py7zr-0.21.0-py3-none-any.whl (67 kB)\n",
      "  Using cached py7zr-0.20.8-py3-none-any.whl (67 kB)\n",
      "  Using cached py7zr-0.20.7-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: pybcj>=0.6.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from py7zr) (1.0.1)\n",
      "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: texttable in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from py7zr) (1.7.0)\n",
      "Requirement already satisfied: brotli>=1.0.9 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: psutil in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from py7zr) (6.1.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from py7zr) (3.21.0)\n",
      "Requirement already satisfied: inflate64>=0.3.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from py7zr) (0.3.1)\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from py7zr) (0.16.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets) (4.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.5.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract transformers datasets rouge-score nltk tensorboard py7zr --upgrade\n",
    "# install git-fls for pushing model and logs to the hugging face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 308M/308M [00:16<00:00, 18.5MB/s] \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20104/1553148885.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Apply the tokenization to the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtokenized_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mtokenized_eval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import Dataset\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load the model and tokenizer\n",
    "model_id = \"google/flan-t5-small\"  # You can change this to a larger model if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Step 2: Preprocess the dataset\n",
    "# Load your dataset\n",
    "dataset_path = \"/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/train_modified.json\"\n",
    "\n",
    "import json\n",
    "\n",
    "with open(dataset_path, 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Convert data into a HuggingFace dataset\n",
    "data = [{'input': item['input'], 'output': item['output'], 'prompt': item['prompt']} for item in data]\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input\": [item[\"input\"] for item in data],\n",
    "    \"output\": [item[\"output\"] for item in data],\n",
    "    \"prompt\": [item[\"prompt\"] for item in data],\n",
    "})\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_dataset, eval_dataset = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "# Step 3: Tokenize the inputs and outputs\n",
    "def preprocess_function(examples):\n",
    "    # Tokenizing input and output\n",
    "    inputs = tokenizer(examples['input'], max_length=512, padding='max_length', truncation=True)\n",
    "    outputs = tokenizer(examples['output'], max_length=128, padding='max_length', truncation=True)\n",
    "\n",
    "    # Add labels to the input\n",
    "    inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply the tokenization to the dataset\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Step 4: Define the training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan_t5_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",  # Optional: directory for storing logs\n",
    ")\n",
    "\n",
    "# Step 5: Define the Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Step 6: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Optionally, save the fine-tuned model\n",
    "trainer.save_model(\"./flan_t5_finetuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a2e255d6fe4157ba2ea9f093e7900e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    " \n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/surenoobster/.cache/huggingface/datasets/json/default-1b5a05f9096b368e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load your dataset from a local JSON file (adjust paths as needed)\n",
    "data_files = {\n",
    "    \"train\": \"/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/train_modified.json\",\n",
    "}\n",
    "\n",
    "# Load dataset from local files\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# Function to format dataset for fine-tuning (prompt + input as the input text)\n",
    "def format_for_finetuning(dataset):\n",
    "    formatted_data = []\n",
    "    for entry in dataset:\n",
    "        # Concatenate the prompt and input to form the input text\n",
    "        input_text = entry[\"prompt\"] + entry[\"input\"]\n",
    "        output_text = entry[\"output\"]  # Target output for the model\n",
    "        \n",
    "        # Add formatted entry to the list\n",
    "        formatted_data.append({\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        })\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/surenoobster/.cache/huggingface/datasets/json/default-1b5a05f9096b368e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|██████████| 1/1 [00:00<00:00, 497.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted dataset saved to: /home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/formatted_train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load your dataset from a local JSON file (adjust paths as needed)\n",
    "data_files = {\n",
    "    \"train\": \"/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/train_modified.json\",\n",
    "}\n",
    "\n",
    "# Load dataset from local files\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# Function to format dataset for fine-tuning (prompt + input as the input text)\n",
    "def format_for_finetuning(dataset):\n",
    "    formatted_data = []\n",
    "    for entry in dataset:\n",
    "        # Concatenate the prompt and input to form the input text\n",
    "        input_text = entry[\"prompt\"] + entry[\"input\"]\n",
    "        output_text = entry[\"output\"]  # Target output for the model\n",
    "        \n",
    "        # Add formatted entry to the list\n",
    "        formatted_data.append({\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        })\n",
    "    return formatted_data\n",
    "\n",
    "# Format the dataset\n",
    "train_data = format_for_finetuning(dataset['train'])\n",
    "\n",
    "# Save the formatted dataset to a new JSON file in the same directory\n",
    "formatted_data_path = \"/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/formatted_train.json\"\n",
    "\n",
    "with open(formatted_data_path, \"w\") as outfile:\n",
    "    json.dump(train_data, outfile, indent=4)\n",
    "\n",
    "print(f\"Formatted dataset saved to: {formatted_data_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 1:\n",
      "Input: Write highlights for this article for a middle school student:\n",
      "\n",
      "(CNN) -- Six people, including three children, were killed when their plane crashed into rugged mountains east of Phoenix on a Thanksgiving eve trip, authorities said Thursday. Pinal County Sheriff Paul Babeu identified the dead as a father and his three children, plus two other men. All were Arizona residents and knew each other well, he said. The twin-engine Rockwell 690A airplane slammed into a steep cliff near the top of the Superstition Mountains at 6:31 p.m. on Wednesday, strewing debris for hundreds of yards down the 80-degree incline, Babeu told reporters. Recovery efforts by more than 50 deputies and volunteers involved collect and preserving the remains, the sheriff said. \"No one could survive that crash,\" he said. The plane had flown from Safford to Mesa, where it picked up the siblings -- ages 9, 8 and 6 -- for the Thanksgiving holiday, Babeu said. \"All of these families are just obviously heartbroken, traumatized over the loss of their loved ones so suddenly here on Thanksgiving,\" he said. Federal officials were expected to begin their investigation as soon as Thursday, according to Babeu. A man who said he witnessed the impact told CNN that a mushroom cloud of flames lit up the evening sky. Kevin Cunningham, 44, of Apache Junction, said he was talking on the phone from his back porch and saw the crash a few miles away. \"The flames lit up the mountain,\" said Cunningham, adding the plane appeared to be flying level before the crash. \"I didn't see where it was in distress.\" CNN's Phil Gast and Tom Cohen contributed to this report.\n",
      "Output: NEW: The dead include three children on a Thanksgiving trip with their father .\n",
      "NEW: \"No one could survive that crash,\" the local sheriff says .\n",
      "A resident says he saw a mushroom cloud of flames .\n",
      "--------------------------------------------------\n",
      "Entry 2:\n",
      "Input: Write highlights for this article for a college student:\n",
      "\n",
      "PUBLISHED: . 12:44 EST, 16 December 2013 . | . UPDATED: . 12:45 EST, 16 December 2013 . Exercise may hold the key to combatting some of the negative side effects that antidepressants can have on a woman’s sex drive. Just 30 minutes of exercise immediately before having sex, ‘significantly’ improves sexual functioning in women taking the drugs and boosts their ability to orgasm, new research suggests. The findings could have important health implications for alleviating some side effects of antidepressants. Exercise may hold the key to combatting some of the negative side effects that antidepressants can have on a woman's sex drive, according to new research by the University of Texas . The study, published online in the journal Depression and Anxiety, shows that sexual dysfunction can be effectively treated with an inexpensive, non-invasive prescription of moderately intense workouts. ‘These findings have important implications for public health, as exercise as a treatment for sexual side effects is accessible, cheap and does not add to burden of care,’ said Tierney Lorenz, an Indiana University post-doctoral research fellow, who conducted the study at the University of Texas at Austin with psychology professor Cindy Meston. The researchers examined 52 women who reported sexual side effects from antidepressants. The study participants had sex without doing an exercise for the first three weeks of the experiment. Over another three weeks, half of the group then exercised for 30 minutes immediately before having sex and the other half did 30 minutes of strength training and cardio exercise three times a week whenever they liked. The two groups then reversed roles in the last experiment. The scientists discovered that regular exercise improved the ability of the women to orgasm. They also found that those who exercised immediately before sex experienced significantly stronger libidos and overall improvements in sexual functioning . Women who exercised regularly were asked to add three extra sessions to their workout routines and all the participants self-reported their sexual function, desires, satisfaction and psychological health before and after each part of the experiment. The scientists discovered that regular exercise improved the ability of the women to orgasm. They also found that those who exercised immediately before sex experienced significantly stronger libidos and overall improvements in sexual functioning. The researchers believe this is because moderately intense exercise activates the sympathetic nervous system, which facilitates blood flow to the genital region, while antidepressants have been shown to decrease this system. Scheduling regular sexual activity and exercise may be an effective tool for alleviating these adverse side effects, Dr Lorenz said. ‘Considering the wide prevalence of antidepressant sexual side effects and the dearth of treatment options for those experiencing these distressing effects, this is an important step in treating sexual dysfunction among women who are taking antidepressants,’ she said. Dr Lorenz said that considering the wide prevalence of antidepressant sexual side effects and the dearth of treatment options for those experiencing these them, the study is an important step in treating sexual dysfunction among women who are taking antidepressants .\n",
      "Output: University of Texas research found regular exercise improves the ability of the women to orgasm .\n",
      "30 minutes of exercise immediately before having sex, 'significantly' improves sexual functioning in women taking antidepressants, they said .\n",
      "May be because exercise boosts blood flow to the genital regions .\n",
      "--------------------------------------------------\n",
      "Entry 3:\n",
      "Input: Write highlights for this article for a high school student:\n",
      "\n",
      "(CNN) -- Pop-cultural suburbia: Take your pick. Happy children. Low taxes. Smooth roads. Picture windows. Quiet streets. Shopping malls. Or: Bored teenagers. Sterile sprawl. Endless traffic. Creepy neighbors. Festering secrets. Shopping malls. For some, the promised land. For others, a worm-infested dullsville. Why such a divide? Perhaps it's the natural reaction to disillusionment, says Syracuse pop culture professor Robert Thompson. The suburb \"was touted as the perfect way to package American life in the utopian era that we expected the end of World War II to bring us,\" he says. \"We're in the Space Age. Television comes out. Air conditioning and dishwashers and all of this stuff. The suburbs stood for the place where this perfect kind of life was going to be lived.\" The suburb wasn't a new concept; people had been attempting to push their way out of crowded cities for as long as there had been crowded cities, and rail service and streetcars made it possible to build communities on city outskirts. \"Streetcar suburbs,\" such as Atlanta's Inman Park, Pittsburgh's Squirrel Hill and Houston's Houston Heights, started sprouting in the 19th century. But it was the automobile, with its promise of personal mobility, that fostered a suburban boom. The most popular exhibit at the 1939 New York World's Fair was Norman Bel Geddes' Futurama -- sponsored by General Motors -- which posited a future of superhighways leading consumers through muscular cities and bucolic towns. \"I Have Seen The Future,\" read the souvenir buttons. CNN Photos: Bill Owens' 'Suburbia' captured the American Dream . The future originally looked something like the past. Tired of six-story walkups? In the suburbs you could have your own picket-fenced home, just like the ones in small towns of yore. Sick of the subway? In the suburbs you could ride the comfortable commuter train -- or, even better, travel to work on a speedy expressway in your very own car. Television was the best mirror of this optimistic age. Mayfield's Cleavers (\"Leave It to Beaver\"), Springfield's Andersons (\"Father Knows Best,\" less biting than the radio version) and Hilldale's Stones (\"The Donna Reed Show\") all lived in crime-free municipalities urban enough to have the latest appliances (like, well, a television) but removed enough that their kids could play and bike without those crowded city worries -- like traffic, ethnic groups and the poor. In this period, it was writers who poked holes in suburban perfection. John Cheever, with his quietly desperate New York escapees, was the master of suburban dissatisfaction, but others -- novelist Richard Yates (\"Revolutionary Road\") and journalist Vance Packard (\"The Status Seekers\") among them -- took aim at the consumerist society entwined with suburbia. On TV, only \"Twilight Zone's\" Rod Serling -- at heart, a writer -- seemed to agree with them. Eventually their views took over. If the cities were rotten -- just check out early '70s films such as \"The French Connection\" and \"Shaft\" for a taste of the chipping paint -- the suburbs were soulless, cookie-cutter places inhabited by people with secrets. Southern California, which is often depicted as one big suburb, was the center of this emptiness (it doesn't hurt that the region's chief industry is fantasy), but it's an image that transfers anywhere, from Chicago (\"Ordinary People\") to Elm Street (\"A Nightmare on Elm Street\"). Indeed, horror movies such as \"Nightmare,\" \"Dawn of the Dead\" and \"Poltergeist\" found plenty to work with in the suburbs. These days, the pop culture divide still holds. To some -- Woody Allen comes to mind -- the city, for all its faults, is vibrant and lively, and the suburbs are \"American Beauty\" dead ends. To others, particularly television producers, it's the suburbs -- where most of us live, after all -- where real life happens, and the city is something for the wealthy, the terminally hip, the criminals and the tourists. iReport: What makes your suburb unique? The truth, of course, is neither here nor there. Today's suburbs are increasingly multicultural and, in some cases, have developed their own skyscraper-filled \"edge cities,\" in the term popularized by journalist Joel Garreau. Accurately portraying such a complex environment is as difficult as accurately portraying the people who live there -- though there are some (author Tom Perrotta, \"My So-Called Life\" producers Marshall Herskovitz and Edward Zwick) who try. In the end, suburbia is what our imagination wants it to be, and it won't quite fit into those little boxes of ticky-tacky. \"We know that inside each one of those identical boxes, with its Dodge parked out front, and its white bread on the table, and its TV set glowing blue in the falling dusk, there were people with stories,\" concluded the pilot of \"The Wonder Years.\" \"There were families bound together in the pain and the struggle of love. There were moments that made us cry with laughter. And there were moments ... of sorrow and wonder.\"\n",
      "Output: Pop-culture depiction of suburbia is divided: Paradise or hell .\n",
      "In years shortly after World War II, suburbia was presented as utopia .\n",
      "In recent decades, attitude has often gone the other way .\n",
      "TV usually favors suburban life; books and movies show negatives .\n",
      "--------------------------------------------------\n",
      "Entry 4:\n",
      "Input: Write highlights for this article for a high school student:\n",
      "\n",
      "Cesc Fabregas was involved in a light training session on Thursday, though it remains unclear whether the Chelsea midfielder will be fit for the title showdown against Manchester City this weekend. The Spaniard, 27, limped off during the Capital One Cup clash with Liverpool with a hamstring problem minutes after colliding with captain John Terry at Stamford Bridge. While Fabregas wasn’t able to take part in a full session with his team-mates at Cobham, he was assessed and is yet to be ruled out of Saturday’s game at home to the Premier League champions. Cesc Fabregas limped out of Chelsea's Capital One Cup semi-final against Liverpool on Tuesday . Fabregas picked up a hamstring problem after this collision with Blues team-mate John Terry . The former Barcelona and Arsenal star has impressed for the Blues this season, racking up 15 assists, the most in the league. He was replaced by Ramires on Tuesday night, who would likely start against Manuel Pellegrini’s side should Fabregas not prove his fitness in time. Mourinho said after beating Liverpool on Wednesday: 'Filipe Luis came off with a calf problem and Fabregas reported a tight hamstring. Fabregas said he wasn't yet injured but that he would be soon if he continued. Meanwhile, the Porguguese boss has called off the pre-match press conference ahead of the crucial tie with City, as he fumes over Diego Costa’s FA charge for violent conduct. Diego Costa, pictured training on Thursday, denies the FA's violent conduct charge for clash with Emre Can . Mourinho is furious with the FA and believes his leading goalscorer is being made a scapegoat following his stamp on Liverpool defender Emre Can. Chelsea will not even send in assistant Steve Holland to speak with the media. Costa has confirmed that he denies the violent conduct charge ahead of Friday's FA disciplinary commission hearing. Despite his protests of innocence, the Chelsea forward is expected to be banned for three games starting with Saturday's clash with City.\n",
      "Output: Cesc Fabregas limped out of Capital One Cup semi-final against Liverpool .\n",
      "Spanish midfielder suffered a hamstring problem and was withdrawn .\n",
      "Fabregas returned to light training at their Cobham base on Thursday .\n",
      "--------------------------------------------------\n",
      "Entry 5:\n",
      "Input: Write highlights for this article for a middle school student:\n",
      "\n",
      "By . Associated Press Reporter . PUBLISHED: . 15:26 EST, 13 December 2013 . | . UPDATED: . 16:31 EST, 13 December 2013 . A 68-year-old Ohio man convicted of fatally shooting his ailing wife in her hospital bed was sentenced today to six years in prison. John Wise has said he shot his debilitated wife Barbara out of love in August 2012 after she suffered aneurysms and appeared to be in pain at an Akron hospital. Mercy is not a defense to a murder charge in Ohio. John Wise (pictured) broke down in tears in court today during his sentencing at Summit County Common Pleas Court in Akron, Ohio . The sentence issued by Summit County Court of Common Pleas Judge Mary Margaret Rowlands was in line with a prosecutor's recommendation that Wise receive a lighter punishment than the minimum 23 years on his most serious conviction, an aggravated murder count with a firearm specification. Wise's attorney, Paul Adamson, said ahead of the sentencing that they will pursue clemency from the governor no matter the punishment. County Prosecutor Sherri Bevan Walsh had called Wise's actions illegal and dangerous but said the case warranted leniency. She had recommended that Wise be sentenced on a lesser offense, manslaughter. Neither side had found previous case law to support the prosecutor's suggestion that the judge could sentence Wise to six years for manslaughter, a charge that wasn't among the counts against him but is considered a lesser included offense. The prosecution instead asked the . judge to sentence Wise for his felonious assault conviction with a . firearms specification, and the judge did so. Wise was also convicted of aggravated murder and murder, which could have led to a life sentence. John Wise, left, is comforted by his attorney Paul Adamson after being sentenced during a hearing at Summit County Common Pleas Court to six years in prison today . Police say Wise calmly walked into his wife's hospital room on Aug. 4, 2012, and shot her at her bedside. She died the next day. Wise told police he intended to kill himself after shooting his wife, but the weapon jammed. 'My recollection is that I walked in there, and within two minutes, I kissed her on the cheek and shot her,' Wise told jurors. Barbara . Wise, 65, was in the intensive care unit at Akron General Medical . Center after suffering triple cerebral aneurysms that had left her . unable to speak, a family friend has said. A doctor testified that Barbara Wise wasn't terminally ill and seemed to be responding to treatment. Wise . testified that he couldn't stand to see his wife of 45 years in pain in . the hospital and decided on his course of action after seeing a tear . roll down her cheek. Those who know Wise described him as a loving husband devastated by his wife's sudden medical emergency. Terry Henderson, a longtime steel . plant co-worker of John Wise, said after the shooting that the couple . had agreed they didn't want to live out their years bedridden and . disabled. John . Wise suffered from diabetes and nerve damage that made his hands and . feet numb and had survived bladder cancer, according to Henderson. Wise calmly walked into his 65-year-old wife Barbara's room at Akron General Medical Center (pictured) and shot her on August 4, 2012 .\n",
      "Output: John Wise, 68, shot dead his wife Barbara, 65, in her Ohio hospital bed last year out of mercy - but mercy is not a defense in Ohio .\n",
      "Barbara suffered a serious stroke in 2012 .\n",
      "John planned on shooting himself after his wife, but the gun jammed .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the newly saved formatted dataset\n",
    "formatted_data_path = \"/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/formatted_train.json\"\n",
    "\n",
    "# Load the formatted dataset from the file\n",
    "with open(formatted_data_path, \"r\") as infile:\n",
    "    formatted_data = json.load(infile)\n",
    "\n",
    "# Print the first few entries to preview the formatted dataset\n",
    "for i, entry in enumerate(formatted_data[:5]):  # Preview first 5 entries\n",
    "    print(f\"Entry {i+1}:\")\n",
    "    print(f\"Input: {entry['input']}\")\n",
    "    print(f\"Output: {entry['output']}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 7fc621c6-5d06-451f-9781-f26858644263)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/spiece.model\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer, model, and data collator\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    " \n",
    "model_id=\"google/flan-t5-base\"\n",
    " \n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_id=\"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/surenoobster/.cache/huggingface/datasets/json/default-f3b513332d46f951/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fa454af98246a59a1985db8b1a3778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 286\n",
      "Input (Prompt + Article): \n",
      "Write highlights for this article for a middle school student:\n",
      "\n",
      "By . Brian Marjoribanks . Ally McCoist insists Sir David Murray is owed an apology from people both ‘in and out of football’ after the ‘needless’ collapse of Rangers in 2012. Last week Murray pointed the finger at the taxman after a judge kicked out HMRC’ s appeal in their long-running case against the Ibrox club. The former owner and chairman said the so-called Big Tax Case case had put off potential buyers and led to Craig Whyte’s disastrous takeover, while fans groups have already called for the SFA, SPFL and other club owners to be held to account over the decision to relegate Rangers when they went bust. Leader: Rangers boss Ally McCoist with players Lee McCulloch (left) and Richard Foster . Still got it: McCoist kicks a ball around during training in California . On Tuesday McCoist said he would ‘never forget’ the role played by leading figures in and around the Scottish game in his club’s descent into financial meltdown. As he called for answers from those he believes are truly responsible for his club’s demise, however, the Rangers boss conceded he won’t be holding his breath for any inquiry or apologies from those involved in the saga. Speaking from his club’s pre-season training base in California, McCoist said: ‘Do people have serious questions to answer? One hundred per cent. I don’t know if I would use the word witch-hunt. But I would definitely say there are certain people that I would hope David Murray would get some answers from in high-up places – both in and out of the game. ‘But I’ll tell you right now - I don’t think we would get an enquiry. And I would be very, very doubtful if David Murray would get an apology from certain people either. ‘But it leaves, at best, a very, very bad taste in my mouth and in the mouths of a lot of people at Rangers when you think of what has happened to this club. ‘It needn’t have happened and it’s horrendous – and I will never forget some of the things which have been said and done to our club by people who should have known better.’ Owed: McCoist believes that former Ranger's chief Sir David Murray deserves an apology . Meltdown: Craig Whyte, who took over from Murray, led the club to the point of ruin . An under-pressure Murray sold Rangers to Craig Whyte for a token £1 in May 2011 at a time when the club was in dispute with HMRC over an Employee Benefit Trust scheme set up a decade before. Rangers subsequently plummeted into administration in February 2012 after the Whyte regime failed to pay tax totalling £14million. HMRC rejected proposals that could have prevented the Ibrox oldco from being liquidated and ultimately demoted to League Two, but the tax man went on to lose the original First Tier Tribunal on the EBT case by a majority verdict in November 2013. HMRC lost an appeal against that verdict last week.\n",
      "---------------\n",
      "Output (Summary): \n",
      "McCoist says Murray is owed an apology after 'needless' Rangers collapse .\n",
      "Gers boss also will 'never forget' those involved in financial meltdown .\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Path to your dataset\n",
    "train_data_path = \"/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/formatted_train.json\"\n",
    "\n",
    "# Load dataset from the local file\n",
    "custom_dataset = load_dataset(\"json\", data_files={\"train\": train_data_path})\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"Train dataset size: {len(custom_dataset['train'])}\")\n",
    "\n",
    "# Preview a random sample\n",
    "from random import randrange\n",
    "\n",
    "sample = custom_dataset['train'][randrange(len(custom_dataset['train']))]\n",
    "print(f\"Input (Prompt + Article): \\n{sample['input']}\\n---------------\")\n",
    "print(f\"Output (Summary): \\n{sample['output']}\\n---------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/surenoobster/.cache/huggingface/datasets/json/default-f3b513332d46f951/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b467fe9ab94cd8bee23a825eadaf38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating max input and target lengths...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6aac5117e445519cf41882a31dccce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bd99731cb54f75bd7ea72271771dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 186\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d199aeadfd84787851afe0e39a3e42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "# Model ID for FLAN-T5\n",
    "model_id = \"google/flan-t5-small\"\n",
    "\n",
    "# Load tokenizer for FLAN-T5\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Path to the formatted dataset\n",
    "train_path = \"/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/formatted_train.json\"\n",
    "\n",
    "# Load the formatted dataset\n",
    "data_files = {\"train\": train_path}\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# Compute maximum input and output lengths for efficient batching\n",
    "print(\"Calculating max input and target lengths...\")\n",
    "\n",
    "# Tokenize the concatenated dataset for inputs\n",
    "tokenized_inputs = dataset[\"train\"].map(\n",
    "    lambda x: tokenizer(x[\"input\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\"input\", \"output\"]\n",
    ")\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# Tokenize the concatenated dataset for targets\n",
    "tokenized_targets = dataset[\"train\"].map(\n",
    "    lambda x: tokenizer(x[\"output\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\"input\", \"output\"]\n",
    ")\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # Prepare inputs with a task prefix (e.g., \"summarize:\")\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"input\"]]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"output\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Replace padding token ID with -100 to ignore during loss computation\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/surenoobster/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, metric, and data collator loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Load FLAN-T5 model\n",
    "model_id = \"google/flan-t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Initialize tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Load ROUGE metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Helper function to postprocess text for evaluation\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # ROUGE expects newlines after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "# Compute metrics function for evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Post-process predictions and labels\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result\n",
    "\n",
    "# Define a DataCollator for Seq2Seq tasks\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=\"max_length\", label_pad_token_id=-100)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Model, tokenizer, metric, and data collator loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 5.77 GiB total capacity; 5.50 GiB already allocated; 11.75 MiB free; 5.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25938/3891773857.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdata_collator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0moptimizers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mpreprocess_logits_for_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_logits_for_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         )\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_model_on_device\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is_loaded_in_8bit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1900\u001b[0m             )\n\u001b[1;32m   1901\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    985\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    986\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 987\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 5.77 GiB total capacity; 5.50 GiB already allocated; 11.75 MiB free; 5.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan_t5_base_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=True,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"no\",\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "\n",
    "# Free memory before training\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: accelerate in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: pyyaml in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from accelerate) (1.21.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: psutil in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: typing-extensions in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: setuptools in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate) (65.6.3)\n",
      "Requirement already satisfied: wheel in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate) (0.38.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n",
      "Device Name: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "CUDA Device Count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"GPU is not available!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readability_summ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
