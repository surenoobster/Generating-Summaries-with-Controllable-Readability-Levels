{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'input_metrics', 'summary', 'id', 'prompt', 'input_noprompt'],\n",
      "    num_rows: 286\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Load your dataset (ensure you have the correct path)\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f.readlines()]\n",
    "    return data\n",
    "\n",
    "# Load the train data\n",
    "train_data = load_json_data('/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/train_metrics_np_summary_prompt_category.json')\n",
    "\n",
    "# Convert list of dictionaries to a dictionary of lists for the dataset format\n",
    "def convert_to_dict_of_lists(data):\n",
    "    dict_data = {}\n",
    "    for entry in data:\n",
    "        for key, value in entry.items():\n",
    "            if key not in dict_data:\n",
    "                dict_data[key] = []\n",
    "            dict_data[key].append(value)\n",
    "    return dict_data\n",
    "\n",
    "train_dict = convert_to_dict_of_lists(train_data)\n",
    "\n",
    "# Create the dataset from the converted dictionary\n",
    "train_dataset = Dataset.from_dict(train_dict)\n",
    "\n",
    "# Optionally print the dataset to verify\n",
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# Copyright 2021 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning the library models for sequence to sequence.\n",
    "\"\"\"\n",
    "# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import nltk  # Here to have a nice missing dependency error message early on\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "import evaluate\n",
    "import transformers\n",
    "from filelock import FileLock\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    MBart50Tokenizer,\n",
    "    MBart50TokenizerFast,\n",
    "    MBartTokenizer,\n",
    "    MBartTokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, is_offline_mode, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "#check_min_version(\"4.25.0.dev0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/summarization/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    if is_offline_mode():\n",
    "        raise LookupError(\n",
    "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
    "        )\n",
    "    with FileLock(\".lock\") as lock:\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# A list of all multilingual tokenizer which require lang attribute.\n",
    "MULTILINGUAL_TOKENIZERS = [MBartTokenizer, MBartTokenizerFast, MBart50Tokenizer, MBart50TokenizerFast]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    resize_position_embeddings: Optional[bool] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to automatically resize the position embeddings if `max_source_length` exceeds \"\n",
    "                \"the model's position embeddings.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    lang: Optional[str] = field(default=None, metadata={\"help\": \"Language id for summarization.\"})\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    text_column: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n",
    "    )\n",
    "    summary_column: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a jsonlines or csv file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"An optional input evaluation data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional input test data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    max_source_length: Optional[int] = field(\n",
    "        default=1024,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_target_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    val_max_target_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n",
    "                \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n",
    "                \"during ``evaluate`` and ``predict``.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to pad all samples to model maximum sentence length. \"\n",
    "                \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n",
    "                \"efficient on GPU but very bad for TPU.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    num_beams: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n",
    "                \"which is used during ``evaluate`` and ``predict``.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    ignore_pad_token_for_loss: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n",
    "        },\n",
    "    )\n",
    "    source_prefix: Optional[str] = field(\n",
    "        default=\"\", metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n",
    "    )\n",
    "\n",
    "    forced_bos_token: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The token to force as the first generated token after the decoder_start_token_id.\"\n",
    "                \"Useful for multilingual models like mBART where the first generated token\"\n",
    "                \"needs to be the target language token (Usually it is the target language token)\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "        if self.val_max_target_length is None:\n",
    "            self.val_max_target_length = self.max_target_length\n",
    "\n",
    "\n",
    "summarization_name_mapping = {\n",
    "    \"amazon_reviews_multi\": (\"review_body\", \"review_title\"),\n",
    "    \"big_patent\": (\"description\", \"abstract\"),\n",
    "    \"cnn_dailymail\": (\"article\", \"highlights\"),\n",
    "    \"orange_sum\": (\"text\", \"summary\"),\n",
    "    \"pn_summary\": (\"article\", \"summary\"),\n",
    "    \"psc\": (\"extract_text\", \"summary_text\"),\n",
    "    \"samsum\": (\"dialogue\", \"summary\"),\n",
    "    \"thaisum\": (\"body\", \"summary\"),\n",
    "    \"xglue\": (\"news_body\", \"news_title\"),\n",
    "    \"xsum\": (\"document\", \"summary\"),\n",
    "    \"wiki_summary\": (\"article\", \"highlights\"),\n",
    "    \"multi_news\": (\"document\", \"summary\"),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_id=\"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    " \n",
    "model_id=\"google/flan-t5-base\"\n",
    " \n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61551aad16db45a7baa4c0471e918c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24415/2706047411.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google/flan-t5-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RMWeerasinghe/flan-t5-base-prompt_tuning-cnn-dailymail\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             return model_class.from_pretrained(\n\u001b[0;32m--> 485\u001b[0;31m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             )\n\u001b[1;32m    487\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2492\u001b[0m                         \u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m                     }\n\u001b[0;32m-> 2494\u001b[0;31m                     \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcached_file_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m                     \u001b[0;31m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         )\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                 \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                 \u001b[0mexpected_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m             )\n\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEffectiveLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     )\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m                 \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "model = PeftModel.from_pretrained(base_model, \"RMWeerasinghe/flan-t5-base-prompt_tuning-cnn-dailymail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/surenoobster/.cache/huggingface/datasets/json/default-1b5a05f9096b368e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a223d9b6204df792e65821d4aebd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load your dataset from a local JSON file (adjust paths as needed)\n",
    "data_files = {\n",
    "    \"train\": \"/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/train_modified.json\",\n",
    "}\n",
    "\n",
    "# Load dataset from local files\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# Function to format dataset for fine-tuning (prompt + input as the input text)\n",
    "def format_for_finetuning(dataset):\n",
    "    formatted_data = []\n",
    "    for entry in dataset:\n",
    "        # Concatenate the prompt and input to form the input text\n",
    "        input_text = entry[\"prompt\"] + entry[\"input\"]\n",
    "        output_text = entry[\"output\"]  # Target output for the model\n",
    "        \n",
    "        # Add formatted entry to the list\n",
    "        formatted_data.append({\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        })\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/surenoobster/.cache/huggingface/datasets/json/default-1b5a05f9096b368e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6220d06ef3433cbcb362161964967a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted dataset saved to: /home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/formatted_train.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load your dataset from a local JSON file (adjust paths as needed)\n",
    "data_files = {\n",
    "    \"train\": \"/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/train_modified.json\",\n",
    "}\n",
    "\n",
    "# Load dataset from local files\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# Function to format dataset for fine-tuning (prompt + input as the input text)\n",
    "def format_for_finetuning(dataset):\n",
    "    formatted_data = []\n",
    "    for entry in dataset:\n",
    "        # Concatenate the prompt and input to form the input text\n",
    "        input_text = entry[\"prompt\"] + entry[\"input\"]\n",
    "        output_text = entry[\"output\"]  # Target output for the model\n",
    "        \n",
    "        # Add formatted entry to the list\n",
    "        formatted_data.append({\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        })\n",
    "    return formatted_data\n",
    "\n",
    "# Format the dataset\n",
    "train_data = format_for_finetuning(dataset['train'])\n",
    "\n",
    "# Save the formatted dataset to a new JSON file in the same directory\n",
    "formatted_data_path = \"/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/formatted_train.json\"\n",
    "\n",
    "with open(formatted_data_path, \"w\") as outfile:\n",
    "    json.dump(train_data, outfile, indent=4)\n",
    "\n",
    "print(f\"Formatted dataset saved to: {formatted_data_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 1:\n",
      "Input: Write highlights for this article for a middle school student:\n",
      "\n",
      "(CNN) -- Six people, including three children, were killed when their plane crashed into rugged mountains east of Phoenix on a Thanksgiving eve trip, authorities said Thursday. Pinal County Sheriff Paul Babeu identified the dead as a father and his three children, plus two other men. All were Arizona residents and knew each other well, he said. The twin-engine Rockwell 690A airplane slammed into a steep cliff near the top of the Superstition Mountains at 6:31 p.m. on Wednesday, strewing debris for hundreds of yards down the 80-degree incline, Babeu told reporters. Recovery efforts by more than 50 deputies and volunteers involved collect and preserving the remains, the sheriff said. \"No one could survive that crash,\" he said. The plane had flown from Safford to Mesa, where it picked up the siblings -- ages 9, 8 and 6 -- for the Thanksgiving holiday, Babeu said. \"All of these families are just obviously heartbroken, traumatized over the loss of their loved ones so suddenly here on Thanksgiving,\" he said. Federal officials were expected to begin their investigation as soon as Thursday, according to Babeu. A man who said he witnessed the impact told CNN that a mushroom cloud of flames lit up the evening sky. Kevin Cunningham, 44, of Apache Junction, said he was talking on the phone from his back porch and saw the crash a few miles away. \"The flames lit up the mountain,\" said Cunningham, adding the plane appeared to be flying level before the crash. \"I didn't see where it was in distress.\" CNN's Phil Gast and Tom Cohen contributed to this report.\n",
      "Output: NEW: The dead include three children on a Thanksgiving trip with their father .\n",
      "NEW: \"No one could survive that crash,\" the local sheriff says .\n",
      "A resident says he saw a mushroom cloud of flames .\n",
      "--------------------------------------------------\n",
      "Entry 2:\n",
      "Input: Write highlights for this article for a college student:\n",
      "\n",
      "PUBLISHED: . 12:44 EST, 16 December 2013 . | . UPDATED: . 12:45 EST, 16 December 2013 . Exercise may hold the key to combatting some of the negative side effects that antidepressants can have on a woman’s sex drive. Just 30 minutes of exercise immediately before having sex, ‘significantly’ improves sexual functioning in women taking the drugs and boosts their ability to orgasm, new research suggests. The findings could have important health implications for alleviating some side effects of antidepressants. Exercise may hold the key to combatting some of the negative side effects that antidepressants can have on a woman's sex drive, according to new research by the University of Texas . The study, published online in the journal Depression and Anxiety, shows that sexual dysfunction can be effectively treated with an inexpensive, non-invasive prescription of moderately intense workouts. ‘These findings have important implications for public health, as exercise as a treatment for sexual side effects is accessible, cheap and does not add to burden of care,’ said Tierney Lorenz, an Indiana University post-doctoral research fellow, who conducted the study at the University of Texas at Austin with psychology professor Cindy Meston. The researchers examined 52 women who reported sexual side effects from antidepressants. The study participants had sex without doing an exercise for the first three weeks of the experiment. Over another three weeks, half of the group then exercised for 30 minutes immediately before having sex and the other half did 30 minutes of strength training and cardio exercise three times a week whenever they liked. The two groups then reversed roles in the last experiment. The scientists discovered that regular exercise improved the ability of the women to orgasm. They also found that those who exercised immediately before sex experienced significantly stronger libidos and overall improvements in sexual functioning . Women who exercised regularly were asked to add three extra sessions to their workout routines and all the participants self-reported their sexual function, desires, satisfaction and psychological health before and after each part of the experiment. The scientists discovered that regular exercise improved the ability of the women to orgasm. They also found that those who exercised immediately before sex experienced significantly stronger libidos and overall improvements in sexual functioning. The researchers believe this is because moderately intense exercise activates the sympathetic nervous system, which facilitates blood flow to the genital region, while antidepressants have been shown to decrease this system. Scheduling regular sexual activity and exercise may be an effective tool for alleviating these adverse side effects, Dr Lorenz said. ‘Considering the wide prevalence of antidepressant sexual side effects and the dearth of treatment options for those experiencing these distressing effects, this is an important step in treating sexual dysfunction among women who are taking antidepressants,’ she said. Dr Lorenz said that considering the wide prevalence of antidepressant sexual side effects and the dearth of treatment options for those experiencing these them, the study is an important step in treating sexual dysfunction among women who are taking antidepressants .\n",
      "Output: University of Texas research found regular exercise improves the ability of the women to orgasm .\n",
      "30 minutes of exercise immediately before having sex, 'significantly' improves sexual functioning in women taking antidepressants, they said .\n",
      "May be because exercise boosts blood flow to the genital regions .\n",
      "--------------------------------------------------\n",
      "Entry 3:\n",
      "Input: Write highlights for this article for a high school student:\n",
      "\n",
      "(CNN) -- Pop-cultural suburbia: Take your pick. Happy children. Low taxes. Smooth roads. Picture windows. Quiet streets. Shopping malls. Or: Bored teenagers. Sterile sprawl. Endless traffic. Creepy neighbors. Festering secrets. Shopping malls. For some, the promised land. For others, a worm-infested dullsville. Why such a divide? Perhaps it's the natural reaction to disillusionment, says Syracuse pop culture professor Robert Thompson. The suburb \"was touted as the perfect way to package American life in the utopian era that we expected the end of World War II to bring us,\" he says. \"We're in the Space Age. Television comes out. Air conditioning and dishwashers and all of this stuff. The suburbs stood for the place where this perfect kind of life was going to be lived.\" The suburb wasn't a new concept; people had been attempting to push their way out of crowded cities for as long as there had been crowded cities, and rail service and streetcars made it possible to build communities on city outskirts. \"Streetcar suburbs,\" such as Atlanta's Inman Park, Pittsburgh's Squirrel Hill and Houston's Houston Heights, started sprouting in the 19th century. But it was the automobile, with its promise of personal mobility, that fostered a suburban boom. The most popular exhibit at the 1939 New York World's Fair was Norman Bel Geddes' Futurama -- sponsored by General Motors -- which posited a future of superhighways leading consumers through muscular cities and bucolic towns. \"I Have Seen The Future,\" read the souvenir buttons. CNN Photos: Bill Owens' 'Suburbia' captured the American Dream . The future originally looked something like the past. Tired of six-story walkups? In the suburbs you could have your own picket-fenced home, just like the ones in small towns of yore. Sick of the subway? In the suburbs you could ride the comfortable commuter train -- or, even better, travel to work on a speedy expressway in your very own car. Television was the best mirror of this optimistic age. Mayfield's Cleavers (\"Leave It to Beaver\"), Springfield's Andersons (\"Father Knows Best,\" less biting than the radio version) and Hilldale's Stones (\"The Donna Reed Show\") all lived in crime-free municipalities urban enough to have the latest appliances (like, well, a television) but removed enough that their kids could play and bike without those crowded city worries -- like traffic, ethnic groups and the poor. In this period, it was writers who poked holes in suburban perfection. John Cheever, with his quietly desperate New York escapees, was the master of suburban dissatisfaction, but others -- novelist Richard Yates (\"Revolutionary Road\") and journalist Vance Packard (\"The Status Seekers\") among them -- took aim at the consumerist society entwined with suburbia. On TV, only \"Twilight Zone's\" Rod Serling -- at heart, a writer -- seemed to agree with them. Eventually their views took over. If the cities were rotten -- just check out early '70s films such as \"The French Connection\" and \"Shaft\" for a taste of the chipping paint -- the suburbs were soulless, cookie-cutter places inhabited by people with secrets. Southern California, which is often depicted as one big suburb, was the center of this emptiness (it doesn't hurt that the region's chief industry is fantasy), but it's an image that transfers anywhere, from Chicago (\"Ordinary People\") to Elm Street (\"A Nightmare on Elm Street\"). Indeed, horror movies such as \"Nightmare,\" \"Dawn of the Dead\" and \"Poltergeist\" found plenty to work with in the suburbs. These days, the pop culture divide still holds. To some -- Woody Allen comes to mind -- the city, for all its faults, is vibrant and lively, and the suburbs are \"American Beauty\" dead ends. To others, particularly television producers, it's the suburbs -- where most of us live, after all -- where real life happens, and the city is something for the wealthy, the terminally hip, the criminals and the tourists. iReport: What makes your suburb unique? The truth, of course, is neither here nor there. Today's suburbs are increasingly multicultural and, in some cases, have developed their own skyscraper-filled \"edge cities,\" in the term popularized by journalist Joel Garreau. Accurately portraying such a complex environment is as difficult as accurately portraying the people who live there -- though there are some (author Tom Perrotta, \"My So-Called Life\" producers Marshall Herskovitz and Edward Zwick) who try. In the end, suburbia is what our imagination wants it to be, and it won't quite fit into those little boxes of ticky-tacky. \"We know that inside each one of those identical boxes, with its Dodge parked out front, and its white bread on the table, and its TV set glowing blue in the falling dusk, there were people with stories,\" concluded the pilot of \"The Wonder Years.\" \"There were families bound together in the pain and the struggle of love. There were moments that made us cry with laughter. And there were moments ... of sorrow and wonder.\"\n",
      "Output: Pop-culture depiction of suburbia is divided: Paradise or hell .\n",
      "In years shortly after World War II, suburbia was presented as utopia .\n",
      "In recent decades, attitude has often gone the other way .\n",
      "TV usually favors suburban life; books and movies show negatives .\n",
      "--------------------------------------------------\n",
      "Entry 4:\n",
      "Input: Write highlights for this article for a high school student:\n",
      "\n",
      "Cesc Fabregas was involved in a light training session on Thursday, though it remains unclear whether the Chelsea midfielder will be fit for the title showdown against Manchester City this weekend. The Spaniard, 27, limped off during the Capital One Cup clash with Liverpool with a hamstring problem minutes after colliding with captain John Terry at Stamford Bridge. While Fabregas wasn’t able to take part in a full session with his team-mates at Cobham, he was assessed and is yet to be ruled out of Saturday’s game at home to the Premier League champions. Cesc Fabregas limped out of Chelsea's Capital One Cup semi-final against Liverpool on Tuesday . Fabregas picked up a hamstring problem after this collision with Blues team-mate John Terry . The former Barcelona and Arsenal star has impressed for the Blues this season, racking up 15 assists, the most in the league. He was replaced by Ramires on Tuesday night, who would likely start against Manuel Pellegrini’s side should Fabregas not prove his fitness in time. Mourinho said after beating Liverpool on Wednesday: 'Filipe Luis came off with a calf problem and Fabregas reported a tight hamstring. Fabregas said he wasn't yet injured but that he would be soon if he continued. Meanwhile, the Porguguese boss has called off the pre-match press conference ahead of the crucial tie with City, as he fumes over Diego Costa’s FA charge for violent conduct. Diego Costa, pictured training on Thursday, denies the FA's violent conduct charge for clash with Emre Can . Mourinho is furious with the FA and believes his leading goalscorer is being made a scapegoat following his stamp on Liverpool defender Emre Can. Chelsea will not even send in assistant Steve Holland to speak with the media. Costa has confirmed that he denies the violent conduct charge ahead of Friday's FA disciplinary commission hearing. Despite his protests of innocence, the Chelsea forward is expected to be banned for three games starting with Saturday's clash with City.\n",
      "Output: Cesc Fabregas limped out of Capital One Cup semi-final against Liverpool .\n",
      "Spanish midfielder suffered a hamstring problem and was withdrawn .\n",
      "Fabregas returned to light training at their Cobham base on Thursday .\n",
      "--------------------------------------------------\n",
      "Entry 5:\n",
      "Input: Write highlights for this article for a middle school student:\n",
      "\n",
      "By . Associated Press Reporter . PUBLISHED: . 15:26 EST, 13 December 2013 . | . UPDATED: . 16:31 EST, 13 December 2013 . A 68-year-old Ohio man convicted of fatally shooting his ailing wife in her hospital bed was sentenced today to six years in prison. John Wise has said he shot his debilitated wife Barbara out of love in August 2012 after she suffered aneurysms and appeared to be in pain at an Akron hospital. Mercy is not a defense to a murder charge in Ohio. John Wise (pictured) broke down in tears in court today during his sentencing at Summit County Common Pleas Court in Akron, Ohio . The sentence issued by Summit County Court of Common Pleas Judge Mary Margaret Rowlands was in line with a prosecutor's recommendation that Wise receive a lighter punishment than the minimum 23 years on his most serious conviction, an aggravated murder count with a firearm specification. Wise's attorney, Paul Adamson, said ahead of the sentencing that they will pursue clemency from the governor no matter the punishment. County Prosecutor Sherri Bevan Walsh had called Wise's actions illegal and dangerous but said the case warranted leniency. She had recommended that Wise be sentenced on a lesser offense, manslaughter. Neither side had found previous case law to support the prosecutor's suggestion that the judge could sentence Wise to six years for manslaughter, a charge that wasn't among the counts against him but is considered a lesser included offense. The prosecution instead asked the . judge to sentence Wise for his felonious assault conviction with a . firearms specification, and the judge did so. Wise was also convicted of aggravated murder and murder, which could have led to a life sentence. John Wise, left, is comforted by his attorney Paul Adamson after being sentenced during a hearing at Summit County Common Pleas Court to six years in prison today . Police say Wise calmly walked into his wife's hospital room on Aug. 4, 2012, and shot her at her bedside. She died the next day. Wise told police he intended to kill himself after shooting his wife, but the weapon jammed. 'My recollection is that I walked in there, and within two minutes, I kissed her on the cheek and shot her,' Wise told jurors. Barbara . Wise, 65, was in the intensive care unit at Akron General Medical . Center after suffering triple cerebral aneurysms that had left her . unable to speak, a family friend has said. A doctor testified that Barbara Wise wasn't terminally ill and seemed to be responding to treatment. Wise . testified that he couldn't stand to see his wife of 45 years in pain in . the hospital and decided on his course of action after seeing a tear . roll down her cheek. Those who know Wise described him as a loving husband devastated by his wife's sudden medical emergency. Terry Henderson, a longtime steel . plant co-worker of John Wise, said after the shooting that the couple . had agreed they didn't want to live out their years bedridden and . disabled. John . Wise suffered from diabetes and nerve damage that made his hands and . feet numb and had survived bladder cancer, according to Henderson. Wise calmly walked into his 65-year-old wife Barbara's room at Akron General Medical Center (pictured) and shot her on August 4, 2012 .\n",
      "Output: John Wise, 68, shot dead his wife Barbara, 65, in her Ohio hospital bed last year out of mercy - but mercy is not a defense in Ohio .\n",
      "Barbara suffered a serious stroke in 2012 .\n",
      "John planned on shooting himself after his wife, but the gun jammed .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the newly saved formatted dataset\n",
    "formatted_data_path = \"/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/formatted_train.json\"\n",
    "\n",
    "# Load the formatted dataset from the file\n",
    "with open(formatted_data_path, \"r\") as infile:\n",
    "    formatted_data = json.load(infile)\n",
    "\n",
    "# Print the first few entries to preview the formatted dataset\n",
    "for i, entry in enumerate(formatted_data[:5]):  # Preview first 5 entries\n",
    "    print(f\"Entry {i+1}:\")\n",
    "    print(f\"Input: {entry['input']}\")\n",
    "    print(f\"Output: {entry['output']}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer, model, and data collator\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    " \n",
    "model_id=\"google/flan-t5-base\"\n",
    " \n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_id=\"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/surenoobster/.cache/huggingface/datasets/json/default-223d9c197d3e5077/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dba006a7df4aa98ff7dceeb68cc7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 286\n",
      "Input (Prompt + Article): \n",
      "Write highlights for this article for a high school student:\n",
      "\n",
      "(CNN) -- Monday's 2009 Major League Baseball home opener for the New York Mets at their new ballpark promises to reignite controversy -- and not just over the team's suspect pitching. Some Mets fans feel the $400 million stadium-name deal is a bad one for all parties in these economic times. Though the Mets played exhibition games there earlier this month, they will officially debut their new home, Citi Field, against the visiting San Diego Padres. The $800 million-plus Queens facility is undoubtedly a state-of-the-art baseball complex with many modern upgrades from the Mets' home for the past 44 years, Shea Stadium, which is still in the process of being demolished a few hundreds yards away. One lifelong Mets fan summed up his emotions about saying goodbye to Shea and hello to Citi by saying: \"It was time for a new stadium, but I'm going to miss Shea very much.\" The controversy arises not from the disappearance of the venerated old stadium but from the fact that rather than attach the iconic Shea name to the new ballpark, in November 2006 the Mets entered into an agreement with Citigroup for naming rights for the new stadium. Reports have put the value of the deal at $400 million spread out in payments of $20 million per year over the course of the next 20 years. That would make it one of the most lucrative stadium naming arrangements in history. According to the Mets, besides the naming rights for Citi Field, \"The fully integrated partnership includes Citi brand and business unit presence throughout the new ballpark.\" While other corporations have invested in similar deals in the past, the financial crisis has focused particular attention on Citigroup, the Mets, and the practice of paying vast sums of money for what is essentially a long-term advertisement. Citigroup, which now does business as Citi, has been the recipient of billions of dollars in taxpayer-funded bailout money over the past year, causing many to question the prudence of $400 million going toward branding Citi Field, especially when Citigroup cut nearly 75,000 jobs in 2008, capped by 50,000 announced in November. Rep. Dennis J. Kucinich, D-Ohio, has been an outspoken critic of the deal. He has called attention to the issue of corporate spending discretion, or lack thereof, particularly when jobs are at risk. \"At the same time they're defending this $400 million stadium-naming deal, they lay off 50,000 people. Now, how many people could you employ for $400 million?\" The Mets claim that the construction of Citi Field created more than 6,000 temporary full-time equivalent jobs, with approximately 1,000 new positions resulting from ongoing operations at the ballpark. One person whose job isn't at risk is Mets all-star third baseman David Wright. One of the franchise's most popular and marketable players, Wright is careful not to make any \"errors\" when confronted with the controversy surrounding his new Citi Field home. \"I don't comment on things that I don't know about. I'm a baseball player, so I go out there and worry about my swing,\" he said.  Watch fans give views on Citi Field name game » . Kucinich conceded that paying to name a stadium is indeed \"great advertising except for one thing, the American taxpayers have invested heavily in these banks and the bailout fund should not be used for this purpose.\" He advocates that the government, now a direct investor in Citigroup, has the obligation to monitor all aspects of how federal bailout funds are used, which in Kucinich's opinion includes any marketing or promotional endeavors. Not all Mets fans agree. One ardent fan said he's happy his hard-earned tax dollars are going to help fund the Mets' new stadium. \"A lot of people pay for advertisements every day. We shouldn't be ashamed of it. Taxpayer money goes to waste on a lot of other things. Let's go Mets, I say!\" But another fan said, \"If it is indeed our tax money that is actually running the stadium, I wish that I wouldn't have to pay as much money to go see a game.\" A group of fellow fans, many liking the sound of a \"Mets rebate\" in these troubling economic times, warmly greeted his idea before an exhibition game against Boston on April 3. David Wright and the rest of the New York Mets, along with 42,000 boisterous fans, will formally open the new ballpark when the first regular-season game there begins at 7 p.m. Monday.\n",
      "---------------\n",
      "Output (Summary): \n",
      "Mets struck deal in 2006 with firm for naming stadium. Price: $400 million .\n",
      "Amid economic climate, some baseball fans have mixed feelings about Citi Field .\n",
      "Rep. Dennis Kucinich: 'How many people could [Citi] employ for $400 million?'\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Path to your dataset\n",
    "train_data_path = \"/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/formatted_train.json\"\n",
    "\n",
    "# Load dataset from the local file\n",
    "custom_dataset = load_dataset(\"json\", data_files={\"train\": train_data_path})\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"Train dataset size: {len(custom_dataset['train'])}\")\n",
    "\n",
    "# Preview a random sample\n",
    "from random import randrange\n",
    "\n",
    "sample = custom_dataset['train'][randrange(len(custom_dataset['train']))]\n",
    "print(f\"Input (Prompt + Article): \\n{sample['input']}\\n---------------\")\n",
    "print(f\"Output (Summary): \\n{sample['output']}\\n---------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/surenoobster/.cache/huggingface/datasets/json/default-223d9c197d3e5077/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a433b64fcdc43998c19b18e0cffc373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/surenoobster/.cache/huggingface/datasets/json/default-223d9c197d3e5077/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d4a255714efdcc5a.arrow\n",
      "Loading cached processed dataset at /home/surenoobster/.cache/huggingface/datasets/json/default-223d9c197d3e5077/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e47ff5bd2bad3f8e.arrow\n",
      "Loading cached processed dataset at /home/surenoobster/.cache/huggingface/datasets/json/default-223d9c197d3e5077/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6492fc3843c42381.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating max input and target lengths...\n",
      "Max source length: 512\n",
      "Max target length: 186\n",
      "Tokenizing dataset...\n",
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "# Model ID for FLAN-T5\n",
    "model_id = \"google/flan-t5-small\"\n",
    "\n",
    "# Load tokenizer for FLAN-T5\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Path to the formatted dataset\n",
    "train_path = \"/home/surenoobster/Documents/controllable-readability-summarization/src/15Dec_realise/formatted_train.json\"\n",
    "\n",
    "# Load the formatted dataset\n",
    "data_files = {\"train\": train_path}\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# Compute maximum input and output lengths for efficient batching\n",
    "print(\"Calculating max input and target lengths...\")\n",
    "\n",
    "# Tokenize the concatenated dataset for inputs\n",
    "tokenized_inputs = dataset[\"train\"].map(\n",
    "    lambda x: tokenizer(x[\"input\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\"input\", \"output\"]\n",
    ")\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# Tokenize the concatenated dataset for targets\n",
    "tokenized_targets = dataset[\"train\"].map(\n",
    "    lambda x: tokenizer(x[\"output\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\"input\", \"output\"]\n",
    ")\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # Prepare inputs with a task prefix (e.g., \"summarize:\")\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"input\"]]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"output\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Replace padding token ID with -100 to ignore during loss computation\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/surenoobster/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, metric, and data collator loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Load FLAN-T5 model\n",
    "model_id = \"google/flan-t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Initialize tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Load ROUGE metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Helper function to postprocess text for evaluation\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # ROUGE expects newlines after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "# Compute metrics function for evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Post-process predictions and labels\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result\n",
    "\n",
    "# Define a DataCollator for Seq2Seq tasks\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=\"max_length\", label_pad_token_id=-100)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Model, tokenizer, metric, and data collator loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1de8b51b006449d834bf2f8478ea931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 593.3607, 'train_samples_per_second': 2.41, 'train_steps_per_second': 0.303, 'train_loss': 2.259277852376302, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./final_model/tokenizer_config.json',\n",
       " './final_model/special_tokens_map.json',\n",
       " './final_model/spiece.model',\n",
       " './final_model/added_tokens.json',\n",
       " './final_model/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan_t5_base_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=False,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"no\",\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "\n",
    "# Free memory before training\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./final_model\")\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (0.20.3)\n",
      "Requirement already satisfied: pyyaml in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: psutil in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from accelerate) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: typing-extensions in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from torch>=1.6.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: wheel in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /home/surenoobster/anaconda3/envs/readability_summ/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate) (65.6.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())  # Number of GPUs available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"GPU is not available!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())  # Number of GPUs available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested Output:\n",
      "Mets will officially debut their new home, Citi Field, against the San Diego Padres. The $800 million-plus Queens facility is undoubtedly a state-of-the-art baseball complex with many modern upgrades from the Mets' home for the past 44 years. Reports have put the value of the deal at $400 million spread out in payments of $20 million per year over the course of the next 20 years.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./final_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./final_model\")\n",
    "\n",
    "# Input text to summarize (as \"Prompt + Article\")\n",
    "input_text = \"\"\"\n",
    "Write highlights for this article for a high school student:\n",
    "\n",
    "(CNN) -- Monday's 2009 Major League Baseball home opener for the New York Mets at their new ballpark promises to reignite controversy -- and not just over the team's suspect pitching. Some Mets fans feel the $400 million stadium-name deal is a bad one for all parties in these economic times. Though the Mets played exhibition games there earlier this month, they will officially debut their new home, Citi Field, against the visiting San Diego Padres. The $800 million-plus Queens facility is undoubtedly a state-of-the-art baseball complex with many modern upgrades from the Mets' home for the past 44 years, Shea Stadium, which is still in the process of being demolished a few hundreds yards away. One lifelong Mets fan summed up his emotions about saying goodbye to Shea and hello to Citi by saying: \"It was time for a new stadium, but I'm going to miss Shea very much.\" The controversy arises not from the disappearance of the venerated old stadium but from the fact that rather than attach the iconic Shea name to the new ballpark, in November 2006 the Mets entered into an agreement with Citigroup for naming rights for the new stadium. Reports have put the value of the deal at $400 million spread out in payments of $20 million per year over the course of the next 20 years. That would make it one of the most lucrative stadium naming arrangements in history. According to the Mets, besides the naming rights for Citi Field, \"The fully integrated partnership includes Citi brand and business unit presence throughout the new ballpark.\" While other corporations have invested in similar deals in the past, the financial crisis has focused particular attention on Citigroup, the Mets, and the practice of paying vast sums of money for what is essentially a long-term advertisement. Citigroup, which now does business as Citi, has been the recipient of billions of dollars in taxpayer-funded bailout money over the past year, causing many to question the prudence of $400 million going toward branding Citi Field, especially when Citigroup cut nearly 75,000 jobs in 2008, capped by 50,000 announced in November. Rep. Dennis J. Kucinich, D-Ohio, has been an outspoken critic of the deal. He has called attention to the issue of corporate spending discretion, or lack thereof, particularly when jobs are at risk. \"At the same time they're defending this $400 million stadium-naming deal, they lay off 50,000 people. Now, how many people could you employ for $400 million?\" The Mets claim that the construction of Citi Field created more than 6,000 temporary full-time equivalent jobs, with approximately 1,000 new positions resulting from ongoing operations at the ballpark. One person whose job isn't at risk is Mets all-star third baseman David Wright. One of the franchise's most popular and marketable players, Wright is careful not to make any \"errors\" when confronted with the controversy surrounding his new Citi Field home. \"I don't comment on things that I don't know about. I'm a baseball player, so I go out there and worry about my swing,\" he said.  Watch fans give views on Citi Field name game » . Kucinich conceded that paying to name a stadium is indeed \"great advertising except for one thing, the American taxpayers have invested heavily in these banks and the bailout fund should not be used for this purpose.\" He advocates that the government, now a direct investor in Citigroup, has the obligation to monitor all aspects of how federal bailout funds are used, which in Kucinich's opinion includes any marketing or promotional endeavors. Not all Mets fans agree. One ardent fan said he's happy his hard-earned tax dollars are going to help fund the Mets' new stadium. \"A lot of people pay for advertisements every day. We shouldn't be ashamed of it. Taxpayer money goes to waste on a lot of other things. Let's go Mets, I say!\" But another fan said, \"If it is indeed our tax money that is actually running the stadium, I wish that I wouldn't have to pay as much money to go see a game.\" A group of fellow fans, many liking the sound of a \"Mets rebate\" in these troubling economic times, warmly greeted his idea before an exhibition game against Boston on April 3. David Wright and the rest of the New York Mets, along with 42,000 boisterous fans, will formally open the new ballpark when the first regular-season game there begins at 7 p.m. Monday.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode and format the summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the summary in the requested format\n",
    "print(\"Requested Output:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 16.7388,  21.6648, -48.9924,  ..., -32.6129, -14.0957, -53.3370],\n",
      "        [ 29.6425,  23.5408,   1.3587,  ..., -21.5251, -24.3179, -33.2913],\n",
      "        [  5.2434, -22.0096, -11.9205,  ...,  -0.9432,  20.6076,  -1.3561],\n",
      "        ...,\n",
      "        [  7.8429,  20.9002,  58.5818,  ..., -66.3809,  26.0704,  23.6928],\n",
      "        [  4.7462,  35.3645,  45.0954,  ..., -16.6990,  -2.6634,   5.6799],\n",
      "        [ 42.1958,  -9.2703,  35.9708,  ...,   8.2285,  38.3363,  20.0297]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Example tensor operation on GPU\n",
    "tensor = torch.randn(1000, 1000).to(device)\n",
    "result = tensor @ tensor\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'allocated_bytes.all.current'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10684/2128386117.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/readability_summ/lib/python3.7/site-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mmemory_summary\u001b[0;34m(device, abbreviated)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_key\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubmetric_key\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"current\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0mpeak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"peak\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mallocated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"allocated\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'allocated_bytes.all.current'"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n",
      "11.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model_path = \"/home/surenoobster/Documents/controllable-readability-summarization/Finetuning_flant5/final_model\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Artificial intelligence is transforming industries worldwide.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Summarize: write it for  Artificial intelligence is transforming industries worldwide.\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "# Generate predictions\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=50,  # Adjust based on your task\n",
    "    num_beams=4,    # Optional: Use beam search for better results\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode the generated tokens\n",
    "predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\", predicted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check the CUDA_VISIBLE_DEVICES environment variable\n",
    "!echo $CUDA_VISIBLE_DEVICES\n",
    "\n",
    "# Alternatively, set it within the notebook\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set to GPU 0\n",
    "!echo $CUDA_VISIBLE_DEVICES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readability_summ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
